\documentclass{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage[margin=41mm]{geometry}

%% Sets page size and margins
%\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{enumitem}

%Theorem
\newtheorem*{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem*{proposition}{Proposition}
\newtheorem*{lemma}{Lemma}
\newtheorem*{definition}{Definition}

%Commands definitions
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\lnorm}[2]{\left\lVert#2 \right\rVert_{#1}}
\newcommand{\inner}[2]{\left\langle#1 , #2 \right\rangle}
\newcommand{\norm}[1]{\left\lVert#1 \right\rVert}
\newcommand{\abs}[1]{\left\lvert#1 \right\rvert}
\newcommand{\Iint}[2]{\llbracket #1 , #2 \rrbracket}
\renewcommand{\Im}{\text{Im}}
\renewcommand{\Re}{\text{Re}}
\newcommand{\card}{\text{card}}
\newcommand{\isomorphic}{\cong}
\newcommand{\td}{\textcolor{red}{\textbf{TODO}}}
\newcommand{\range}{\text{range}}

%Example environment
\newenvironment{example}{\noindent\textbf{Example:} \vspace{-0.2cm}\begin{itemize}}{\end{itemize}}

%Remark environment
\newenvironment{remark}{\noindent\textbf{Remark:}}{}

%Set QED symbol to blacksquare
\renewcommand\qedsymbol{$\blacksquare$}


\title{MATH 565 Notes : Functional Analysis}
\author{Samy Lahlou}
\date{}

\begin{document}

\maketitle

These notes are based on lectures given by Professor John Toth at McGill University in Winter 2025. The subject of these lectures is Functional Analysis, the prerequisites are Real Analysis and Linear Algebra. \\
As a disclaimer, it is more than possible that I made some mistakes. Feel free to correct me or ask me anything about the content of this document at the following address : samy.lahloukamal@mcgill.ca

\tableofcontents

\newpage

\section{Normed Linear Spaces}

In these notes, consider that the field $\F$ is either $\R$ or $\C$ if it's not specified.

\begin{definition}[Semi-norm]
    Given a vector space $X$ over $\F$, we call the function $\|\cdot\| : X \to [0, \infty)$ a semi-norm if
    \begin{enumerate}
        \item $\|x + y\| \leq \|x\| + \|y\|$ for all $x,y\in X$.
        \item $\|\lambda x\| = |\lambda| \|x\|$ for all $\lambda \in \F$ and $x \in X$.
    \end{enumerate}
    The second condition is called the \textit{Triangle Inequality}.
\end{definition}

\begin{definition}[Norm \& Normed Linear Space]
    Given a vector space $X$ over $\F$, we call the function $\|\cdot\| : X \to [0, \infty)$ a norm if it is a semi-norm that satisfies the following additional condition: 
    \begin{enumerate}
        \item $\|x\| = 0 \implies x = 0$ for all $x\in X$.
    \end{enumerate}
    In that case, we call $(X, \|\cdot\|)$ a Normed Linear Space (NLS).
\end{definition}

\begin{example}
    \item All finite-dimesnional vector spaces $V$ are NLS using the isomorphism $V \isomorphic \F^n$ where $n = \dim V$.
    \item Given a measure space $(X, \M, \mu)$, we can define the norm $\lnorm{1}{\cdot}$ on $L^1(\mu)$ by
    $$\lnorm{1}{f} = \int_X |f| d\mu.$$
    It is a semi-norm because the integral is linear and because the triangle inequality holds in $\F$ with $|\cdot|$. It is a norm because
    \begin{align*}
        \lnorm{1}{f} = 0 &\implies \int_X |f| d\mu = 0 \\
        &\implies |f| = 0 \ \ \mu.a.e \\
        &\implies f = 0 \ \ \mu.a.e \\
        &\implies f = 0 \ \ \text{in } L^1(\mu)
    \end{align*}
    It follows that $L^1(\mu)$ is a NLS.
\end{example}

\begin{definition}[Equivalent Norms]
    Given a vector space $X$ and two norms $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ on $X$, we say that the two norms are equivalent if there exist constants $C_1, C_2 > 0$ such that 
    $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
    for all $x \in X$.
\end{definition}

\begin{proposition}
    Given a vector space $X$, define the relation $\sim$ on the norms on $X$ as follows: $\lnorm{1}{\cdot} \sim \lnorm{2}{\cdot}$ if and only if there exist constants $C_1, C_2 > 0$ such that 
    $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
    for all $x \in X$, then $\sim$ is an equivalence relation.
\end{proposition}

\begin{proof} \leavevmode
    \begin{itemize}
        \item (Reflexive) Let $\norm{\cdot}$ be a norm on $X$, then $\norm{\cdot} \sim \norm{\cdot}$ by setting $C_1, C_2 > 0$.
        \item (Symmetric) Let $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ be norms on $X$ such that $\lnorm{1}{\cdot} \sim \lnorm{2}{\cdot}$, then there exist constants $C_1, C_2 > 0$ such that 
        $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
        for all $x \in X$. Notice that for all $x \in X$, we have
        \[C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \implies \lnorm{1}{x} \leq \frac{1}{C_1}\lnorm{2}{x} \tag*{(1)} \]
        and 
        \[\lnorm{2}{x} \leq C_2 \lnorm{1}{x} \implies \frac{1}{C_2}\lnorm{2}{x} \leq \lnorm{1}{x}.\tag*{(2)} \]
        Thus, combining (1) and (2) gives us
        $$\frac{1}{C_2}\lnorm{2}{x} \leq \lnorm{1}{x} \leq \frac{1}{C_1}\lnorm{2}{x}$$
        for all $x \in X$. Therefore, $\lnorm{2}{\cdot} \sim \lnorm{1}{\cdot}$.
        \item (Transitive) Let $\lnorm{1}{\cdot}, \lnorm{2}{\cdot}, \lnorm{3}{\cdot}$ be norms on $X$ such that $\lnorm{1}{\cdot} \sim \lnorm{2}{\cdot}$ and $\lnorm{2}{\cdot} \sim \lnorm{3}{\cdot}$, then there exist constants $C_1, C_2, C_3, C_4 > 0$ satisfying
        $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
        and 
        $$C_3 \lnorm{2}{x} \leq \lnorm{3}{x} \leq C_4 \lnorm{2}{x}$$
        for all $x \in X$. It follows that
        $$C_3 C_1 \lnorm{1}{x} \leq C_3 \lnorm{2}{x} \leq \lnorm{3}{x} \leq C_4 \lnorm{2}{x} \leq C_4 C_2 \lnorm{1}{x}$$
        for all $x \in X$. Therefore, $\lnorm{1}{\cdot} \sim \lnorm{3}{\cdot}$
    \end{itemize}
\end{proof}

\begin{proposition}
    Given a NLS $(X, \norm{\cdot})$, the function $d : X^2 \to [0, \infty)$ defined by $d(x,y) = \norm{x - y}$ is a metric on $X$.
\end{proposition}

\begin{proof}
    First, let's show that $d(x,y) = 0$ if and only if $x = y$. To do so, suppose that $d(x,y) = 0$, then by definition, $\norm{x - y} = 0$ which directly implies $x - y = 0$, which is equivalent to $x = y$. Suppose now that $x = y$, then 
    $$d(x,y) = \norm{x - y} = \norm{0} = 0.$$
    Take $x,y \in X$, and notice that
    \begin{align*}
        d(x,y) &= \norm{ x- y}\\
        &= \norm{(-1)(y - x)} \\
        & = |-1|\norm{y - x} \\
        &= d(y,x).
    \end{align*}
    Finally, for any $x,y,z \in X$, we have
    \begin{align*}
        d(x,y) &= \norm{x - y} \\
        &= \norm{x - z + z - y} \\
        &\leq \norm{x - z} + \norm{z - y} \\
        &= d(x,z) + d(z,y).
    \end{align*}
    Therefore, $d$ is a metric on $X$.
\end{proof}

\begin{definition}[Induced Topology]
    A NLS $(X, \norm{\cdot})$ induces a metric space $(X, d)$ where the metric $d : X \times X \to [0, \infty)$ is defined by $d(x,y) = \norm{x - y}$. This metric space induces a topological space $(X, \tau)$ where $\tau$ is generated by the open balls of $(X, d)$. We call $\tau$ the topology induced by the norm $\norm{\cdot}$.
\end{definition}

\begin{proposition}
    Let $X$ be a vector space and $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ be two norms on $X$. If the norms are equivalent, then they induce the same topology on $X$.
\end{proposition}

\begin{proof}
    Since $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ are equivalent, then there exist constants $C_1, C_2 > 0$ such that
    $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
    for all $x \in X$. Let $T_1$ and $T_2$ be the sets of open balls in the metric spaces induced by $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ respectively:
    $$T_1 = \{B_{\epsilon}^{(1)}(x) : \epsilon > 0 \text{ and } x\in X\}$$
    $$T_2 = \{B_{\epsilon}^{(2)}(x) : \epsilon > 0 \text{ and } x\in X\}$$
    where 
    $$B_{\epsilon}^{(1)}(x) = \{y \in X : \lnorm{1}{x - y} < \epsilon\}$$
    and 
    $$B_{\epsilon}^{(2)}(x) = \{y \in X : \lnorm{2}{x - y} < \epsilon\}.$$
    Moreover, we know that the topology induced by $\lnorm{1}{\cdot}$ is $\tau_1 = \langle T_1 \rangle$ and the topology induced by $\lnorm{2}{\cdot}$ is $\tau_2 = \langle T_2 \rangle$. Let's show that $\tau_1 = \tau_2$. To do this, let $\epsilon > 0$ and $x \in X$ be arbitrary, let's show that $B_{\epsilon}^{(1)}(x)$ is open in the topology $\tau_2$. For all $z \in B_{\epsilon}^{(1)}(x)$, there exists a $\delta_z > 0$ such that $B_{\delta_z}^{(1)}(z) \subset B_{\epsilon}^{(1)}(x)$. Let's prove that
    $$B_{\epsilon}^{(1)}(x) = \bigcup_{z \in  B_{\epsilon}^{(1)}(x)}B_{C_1\delta_z}^{(2)}(z).$$
    First, if $y \in B_{\epsilon}^{(1)}(x)$, then $y \in B_{C_1\delta_y}^{(2)}(y)$ which implies that $y \in \bigcup_{z \in  B_{\epsilon}^{(1)}(x)}B_{C_1\delta_z}^{(2)}(z)$. Moreover, if $y \in \bigcup_{z \in  B_{\epsilon}^{(1)}(x)}B_{C_1\delta_z}^{(2)}(z)$, then $y \in B_{C_1\delta_z}^{(2)}(z)$ for some $z \in B_{\epsilon}^{(1)}(x)$. It follows that
    \begin{align*}
        y \in B_{C_1\delta_z}^{(2)}(z) &\implies \lnorm{2}{z - y} < C_1 \delta_z \\
        &\implies C_1 \lnorm{1}{z - y} < C_1 \delta_z \\
        &\implies \lnorm{1}{z - y} < \delta_z \\
        &\implies y \in B_{\delta_z}^{(1)}(z) \\
        &\implies y \in B_{\epsilon}^{(1)}(x). 
    \end{align*}
    Therefore, $B_{\epsilon}^{(1)}(x)$ can be written as a union of open sets in $\tau_2$, so $B_{\epsilon}^{(1)}(x)$ is itself open in $\tau_2$. Since it holds for all $\epsilon > 0$ and $x \in X$, then $T_1 \subset \tau_2$. Thus, it follows that $\tau_1 \subset \tau_2$. Proving the reverse inclusion is very similar.
\end{proof}

\section{Banach Spaces}

\begin{definition}[Banach Space]
    A NLS $(X, \norm{\cdot})$ is complete if its induced metric space is complete, i.e., every Cauchy sequence converge. We call such a NLS a Banach Space. When it is clear from the context, we can directly write that $X$ is complete or that $X$ is a Banach Space.
\end{definition}

\begin{theorem}[Completeness Criterion]
    Let $(X, \norm{\cdot})$ be a NLS, then $X$ is complete if and only if every absolutely convergent series converges in $X$.
\end{theorem}

\begin{proof}
    ($\implies$) Suppose $X$ is complete and let $\{x_n\}_n$ be a sequence such that
    $$\sum_{k=1}^{\infty}\norm{x_k} < \infty.$$
    Set $S_n = \sum_{k=1}^{n}x_k \in X$ and notice that for all $\epsilon > 0$, by convergence of the series $\sum_{k=1}^{\infty}\norm{x_k}$, there is a $N \in \N$ such that for all $m > n \geq N$, we have
    $$\abs{\sum_{k=1}^{m}\norm{x_k} - \sum_{k=1}^{n}\norm{x_k}} < \epsilon.$$
    But since the norm is positive and $m > n$, then we can simply rewrite the previous inequality as
    $$\sum_{k=n+1}^{m}\norm{x_k} < \epsilon.$$
    Thus, for all $m > n \geq N$, by the triangle inequality,
    $$\norm{S_m - S_n} = \norm{\sum_{k=1}^{m}x_k - \sum_{k=1}^{n}x_k} \leq \sum_{k=n+1}^{m}\norm{x_k} < \epsilon$$
    Therefore, $\{S_n\}_n$ is a Cauchy sequence so by completeness, there is a $x \in X$ such that $\sum_{k=1}^{\infty}x_k = x \in X$. \\
    $( \ \Longleftarrow \ )$ Suppose that any absolutely convergent series converges in $X$, we want to show that $X$ is complete. To do so, let $\{x_n\}_n$ be a Cauchy sequence in $X$, then we can define recursively a sequence of positive integers $\{n_k\}_k$ as follows: Since $\{x_n\}_n$ is Cauchy, then there is an integer $n_1$ such that $\norm{x_m - x_n} \leq 2^{-1}$ for all $m > n \geq n_1$. Similarly, if $n_k$ is defined, then we can define $n_{k+1}$ as follows: Since $\{x_n\}_n$ is Cauchy, then there is an integer $N$ such that $\norm{x_m - x_n} \leq 2^{-(k+1)}$ for all $m > n \geq N$, define $n_{k+1}$ as the maximum between $N$ and $n_k + 1$. Therefore, the sequence $\{n_k\}_k$ is strictly increasing. \\
    From this sequence, define the sequence $\{y_j\}_j$ by $y_1 = x_{n_1}$ and $y_j = x_{n_j} - x_{n_{j-1}}$ for $j \geq 2$. Then by construction, for all positive integers $k$, we have $\sum_{j=1}^{k}y_j = x_{n_k}$. Moreover,
    \begin{align*}
        \sum_{k=1}^{\infty}\norm{y_k} &= \norm{y_1} + \sum_{k=2}^{\infty}\norm{x_{n_k} - x_{n_{k-1}}} \\
        &= \norm{y_1} + \sum_{k=1}^{\infty}\norm{x_{n_{k+1}} - x_{n_k}} \\
        &\leq \norm{y_1} + \sum_{k=1}^{\infty}2^{-k} \\
        &= \norm{y_1} + 1\\
        &< \infty.
    \end{align*}
    So by our assumption, the series $\sum y_k$ converges to a $x \in X$. But since $\sum_{j=1}^{k}y_j = x_{n_k}$ for all $k$, then the sequence $\{x_{n_k}\}_k$ must converge to $x$ as well. Thus a subsequence of $\{x_n\}_n$ converges in $X$ so $\{x_n\}_n$ must converge as well since it is Cauchy. Therefore, $X$ is complete.
\end{proof}

\begin{theorem}[$L^1$ is Banach]
    Let $(X, \M, \mu)$ be a measure space, then $L^1(\mu)$ is Banach.
\end{theorem}

\begin{proof}
    Let's apply the Completeness Criterion stated and proved right above. Suppose $\{f_n\}_n$ is a sequence of functions such that 
    $$\sum_{n=1}^{\infty}\lnorm{1}{f_n} < \infty,$$
    then by the Monotone Convergence Theorem, 
    $$\int_X \sum_{n=1}^{\infty}|f_n|d \mu = \sum_{n=1}^{\infty}\lnorm{1}{f_n} < \infty$$
    so $\sum_{n=1}^{\infty}|f_n|$ is finite a.e. Hence, the function $f : x \mapsto \sum_{n=1}^{\infty}|f_n(x)|$ exists on $X$ (if the series diverges for a $x$, let $f(x)$ be zero). Moreover, by the Monotone Convergence Theorem, $f \in L^1(\mu)$. To show that $f_n \to f$ in $L_1(\mu)$, simply notice that
    \begin{align*}
        \lim_{n \rightarrow \infty}\lnorm{1}{f - f_n} &= \lim_{n \rightarrow \infty}\int_X \abs{\sum_{j=1}^{\infty}|f_j| - \sum_{j=1}^{n}|f_j|} d\mu \\
        &= \lim_{n \rightarrow \infty}\int_X \sum_{j=n+1}^{\infty}|f_j| d\mu \\
        &= \lim_{n \rightarrow \infty} \sum_{j=n+1}^{\infty} \int_X |f_j| d\mu \\
        &= \lim_{n \rightarrow \infty} \sum_{j=n+1}^{\infty} \lnorm{1}{f_j}\\
        &= 0.
    \end{align*}
    Therefore, all absolutely convergent series converge in $X$ so by the Completeness Criterion, $L^1(\mu)$ is Banach.
\end{proof}

\section{Bounded Operators}

\begin{definition}[Boundedness]
    Let $T$ be a linear operator between the NLS $(X, \lnorm{X}{\cdot})$ and $(Y, \lnorm{Y}{\cdot})$. If there is a constant $C > 0$ such that
    $$\lnorm{Y}{Tx} \leq C \lnorm{X}{x}$$
    for all $x \in X$, then we say that $T$ is bounded. The set of all bounded operators from $X$ to $Y$ is denoted by $\L(X,Y)$.
\end{definition}

\begin{theorem}[Equivalence between Continuity and Boundedness]
    Let $T$ be a linear operator between the NLS $(X, \lnorm{X}{\cdot})$ and $(Y, \lnorm{Y}{\cdot})$, then $T$ is continuous if and only if $T$ is bounded.
\end{theorem}

\begin{proof}
    $( \implies )$ Suppose that $T$ is continuous, then for all $y \in X$ and $\epsilon > 0$, there is a $\delta > 0$ such that $\lnorm{X}{Tx - Ty} \leq \epsilon$ for all $x \in X$ satisfying $\lnorm{X}{x - y} \leq \delta$. In particular, for $y = 0$ and $\epsilon = 1$, we get that there exists a $\delta > 0$ such that
    $$\lnorm{X}{x} \leq \delta \implies \lnorm{Y}{Tx} \leq 1$$
    for all $x \in X$. Define $C = 1/\delta$ and notice that for all $x \in X$, we have
    $$\lnorm{X}{\delta\frac{x}{\lnorm{X}{x}}} \leq \delta,$$
    Thus, we obtain:
    \begin{align*}
        \lnorm{Y}{Tx} &= \lnorm{Y}{\frac{\lnorm{X}{x}}{\delta}T\left(\delta\frac{x}{\lnorm{X}{x}}\right)} \\
        &= C\lnorm{X}{x} \lnorm{Y}{T\left(\delta\frac{x}{\lnorm{X}{x}}\right)} \\
        &\leq C \lnorm{X}{x}.
    \end{align*}
    Therefore, $T$ is bounded. \\
    $( \ \Longleftarrow \ )$ Suppose that $T$ is bounded, then there exists a constant $C > 0$ such that
    $$\lnorm{Y}{Tx} \leq C \lnorm{X}{x}$$
    for all $x \in X$. Take an arbitrary $x \in X$, $\epsilon > 0$ and $y \in X$ such that $\lnorm{X}{x - y} \leq \epsilon/C$, then
    $$\lnorm{Y}{Tx - Ty} = \lnorm{Y}{T(x - y)} \leq C\lnorm{X}{x - y} \leq \epsilon.$$
    Therefore, $T$ is continuous.
\end{proof}

\begin{proposition}[$\L$ is a vector space]
    Let $X$ and $Y$ be NLS, then $\L(X,Y)$ is a vector space.
\end{proposition}

\begin{proof}
    Since $\L(X,Y)$ is a subset of the vector space $Y^X$, it suffices to show that $\L(X,Y)$ is a subspace. First, it is non-empty since the constant zero function is a bounded linear operator. Moreover, given $T_1, T_2 \in \L(X,Y)$, there exist constants $C_1, C_2 > 0$ such that
    $$\norm{T_1x} \leq C_1 \norm{x} \quad \text{ and } \quad \norm{T_2x} \leq C_2 \norm{x}$$
    for all $x \in X$. Thus, for all $x \in X$, we have
    $$\norm{(T_1 + T_2)x} \leq \norm{T_1x} + \norm{T_2x} \leq (C_1 + C_2)\norm{x}.$$
    Hence, $T_1 + T_2$ is bounded so $\L(X,Y)$ is closed under addition. Finally, for all $\lambda \in \F$ and $T \in \L(X,Y)$, there exists a constant $C > 0$ such that
    $$\norm{Tx} \leq C \norm{x}$$
    for all $x \in X$. If $\lambda = 0$, then $\lambda T = 0 \in \L(X,Y)$, and if $\lambda \neq 0$, then for all $x \in X$, 
    $$\norm{\lambda Tx} \leq |\lambda| C \norm{x}.$$  
    Hence, $\lambda T $ is bounded so $\lambda T \in \L(X,Y)$. Therefore, $\L(X,Y)$ is a vector space since it is a subspace.  
\end{proof}

\begin{definition}[Operator Norm]
    Given two NLS $X$ and $Y$, we can define the operator norm on $\L(X,Y)$ as follows:
    $$\norm{T} = \sup_{\norm{x} = 1}\norm{Tx} = \sup_{x \neq 0}\frac{\norm{Tx}}{\norm{x}}$$
    for all $T \in \L(X,Y)$.
\end{definition}

\begin{proposition}[$\L$ is a NLS]
    Let $X$ and $Y$ be NLS, then $\L(X,Y)$ is a NLS with the operator norm.
\end{proposition}

\begin{proof}
    We already proved that $\L(X,Y)$ is a vector space so it suffices to prove that the operator norm is indeed a norm on $\L(X,Y)$. To do so, let's first show that $\norm{T} = 0$ if and only if $T = 0$. If $T = 0$, then obviously:
    $$\norm{T} = \sup_{\norm{x} = 1}\norm{0 \cdot x} = 0.$$
    Suppose now that $\norm{T} = 0$ and let $\{b_i\}_{i \in I}$ be a basis for $X$ (which exists by the axiom of choice), then we can assume that all $b_i$'s have norm 1 (otherwise, replace $b_i$ by $b_i / \norm{b_i}$). Hence, for all $i \in I$, we have
    $$\sup_{\norm{x} = 1}\norm{Tx} = 0 \implies \norm{T b_i} = 0 \implies Tb_i = 0.$$
    Since $T$ is identically zero on a basis of $X$, then $T = 0$. This proves the first norm axiom for the operator norm. \\
    Next, take $\lambda \in \F$ and $T \in \L(X,Y)$, then by properties of the supremum, we have
    \begin{align*}
        \norm{\lambda T} &= \sup_{\norm{x} = 1}\norm{\lambda T x} \\
        &= \sup_{\norm{x} = 1}|\lambda|\norm{T x} \\
        &= \lambda \sup_{\norm{x} = 1}\norm{T x} \\
        &= \lambda \norm{T}.
    \end{align*}
    Finally, for all $T_1, T_2 \in \L(X,Y)$ and $x \in X$ with $\norm{x} = 1$, we know that
    $$T_1 x \leq \sup_{\norm{x} = 1}\norm{T_1 x} = \norm{T_1}$$
    and 
    $$T_2 x \leq \sup_{\norm{x} = 1}\norm{T_2 x} = \norm{T_2}.$$
    Thus,
    $$(T_1 + T_2)x = T_1x + T_2x \leq \norm{T_1} + \norm{T_2}.$$
    Since it holds for all $x \in X$ with $\norm{x} = 1$, then
    $$\norm{T_1 + T_2} \leq \norm{T_1} + \norm{T_2}.$$
    Therefore, $\L(X,Y)$ is a NLS. 
\end{proof}

\begin{proposition}
    Let $X$ and $Y$ be NLS, $T \in \L(X,Y)$ and $x \in X$, then
    $$\norm{Tx} \leq \norm{T}\norm{x}$$
\end{proposition}

\begin{proof}
    If $x = 0$, it is trivial. Hence, assume that $x \neq 0$, then
    $$\frac{\norm{Tx}}{\norm{x}} \leq \sup_{y \neq 0}\frac{\norm{Ty}}{\norm{y}} = \norm{T}.$$
    Thus, multiplying on both sides by $\norm{x}$ gives us
    $$\norm{Tx} \leq \norm{T}\norm{x}.$$
\end{proof}

\begin{theorem}[$\L$ is Banach]
    Assume $X$ is a NLS and $Y$ is Banach, then $\L(X,Y)$ is Banach. 
\end{theorem}

\begin{proof}
    Let $\{T_n\}_n$ be a Cauchy sequence in $\L(X,Y)$, then for all $\epsilon > 0$, there is a natural number $N$ such that $\norm{T_m - T_n} < \epsilon$ for all $m > n \geq N$. Thus, for all $x \in X$ and $\epsilon > 0$, there is a natural number $N$ such that $\norm{T_m - T_n} < \epsilon/\norm{x}$ for all $m > n \geq N$. Hence, for all $m > n \geq N$, we obtain
    $$\norm{T_m x - T_n x} \leq \norm{T_m - T_n} \norm{x} < \epsilon.$$
    Therefore, $\{T_nx\}_n$ is a Cauchy sequence in $Y$ for all $x \in X$. Thus, by completeness of $Y$, define the function $T : X \to Y$ such that $Tx = \lim_n T_n x$ for all $x \in X$. Using properties of limits and the linearity of the $T_n$'s, we easily obtain that $T$ is linear. \\
    Let's show that $T$ is bounded. To do so, we will prove that $\lim_n \|T_n - T\| = 0$. \td 
\end{proof}

\begin{proposition}
    Let $X,Y,Z$ be NLS, $T \in \L(X,Y)$ and $S \in \L(Y,Z)$, then $$\|ST\| \leq \|S\|\|T\|.$$
\end{proposition}

\begin{proof}
    Simply notice that 
    $$\|ST\| = \sup_{\|x\| = 1}\|STx\| \leq \sup_{\|x\| = 1}\|S\|\|Tx\| = \|S\| \|T\|.$$
\end{proof}

\begin{definition}[Invertible]
    Let $T \in \L(X,Y)$, we say that $T$ is invertible if it is a bijection and if $T^{-1} \in \L(Y,X)$.
\end{definition}

\begin{definition}[Isometry]
    Let $T \in \L(X,Y)$, we say that $T$ is an isometry if it is invertible and $\|Tx\| = \|x\|$ for all $x \in X$.
\end{definition}

\section{Linear Functionals}
\begin{definition}[Linear Functionals]
    Let $X$ be a vector space, then we define $X^*_{\F}$ as the set of all linear transformations from $X$ to $\F$ and call it the space of $\F$-linear functionals.
\end{definition}

\begin{proposition}
    Let $X$ be a $\C$-vector space and let $f : X \to \C$ be a $\C$-linear functional, then $u = \Re f$ is a $\R$-linear functional and $f(x) = u(x) - iu(ix)$ for all $x \in X$. Conversely, if $u$ is $\R$-linear functional, then we can construct a $\C$-linear functional $f : X \to \C$ as follows: $f(x) = u(x) - iu(ix)$. Finally, if $X$ is a NLS, $\norm{u} = \norm{f}$.
\end{proposition}

\begin{proof}
    $(\implies)$ Let $f$ be a $\C$-linear functional, then $f$ is $\R$-linear. It follows that $u = \Re f$ is $\R$-linear as well so $u$ is a $\R$-linear functional. Moreover, using the identity $\Im z = - \Re(iz)$, we get that
    $$f(x) = \Re (f(x)) + i \Im (f(x)) = u(x) - i u(ix).$$
    $( \ \Longleftarrow \ )$ Suppose now that $u$ is a $\R$-linear functional, set $f(x) = u(x) - iu(ix)$ for all $x \in X$, then by $\R$-linearity of $u$, we have that $f$ is $\R$-linear as well. To prove that it is $\C$-linear, it suffices to prove that $f(ix) = if(x)$:
    \begin{align*}
        f(ix) &= u(ix) - iu(i(ix)) \\
        &= u(ix) + iu(x) \\
        &= i(u(x) - iu(ix)) \\
        &= if(x).
    \end{align*}
    Therefore, $f$ is a $\C$-linear functional.

    Finally, suppose that $X$ is a NLS, then we easily get that
    $$\|u\| = \sup_{\|x\| = 1}|u(x)| \leq \sup_{\|x\| = 1}|f(x)| \leq \|f\|.$$
    Let's prove the reverse inequality. If $f(x) \neq 0$, define $\alpha = \frac{\overline{f(x)}}{|f(x)|}$ and notice that,
    $$|\alpha| = \abs{\frac{\overline{f(x)}}{|f(x)|}} = \frac{|f(x)|}{|f(x)|} = 1.$$
    Moreover, by $\C$-linearity, we have
    $$f(\alpha x) = \alpha f(x) = \frac{\overline{f(x)}}{|f(x)|}f(x) = \frac{|f(x)|^2}{|f(x)|} = |f(x)|.$$
    Therefore, since $f(\alpha x) \in \R$, then 
    $$u(\alpha x) = f(\alpha x) = |f(x)|.$$
    It follows that
    $$\norm{f} = \sup_{\|x\| = 1}|f(x)| = \sup_{\|x\| = 1} u(\alpha x) \leq \sup_{\|x\| = 1} |u(\alpha x)| = \|u\|.$$
    The last inequality follows from the fact that $\|\alpha x\| = |\alpha| \norm{x} = \norm{x}$ so 
    $$\{|u(\alpha x)| : \norm{x} = 1\} = \{|u(x)| : \norm{x} = 1\}.$$
    Therefore, $\norm{u} = \norm{f}$.
\end{proof}

\begin{definition}[Sublinear Functional]
    Let $X$ be a $\R$-vector space. A sublinear functional is a map $p : X \to \R$ satisfying
    \begin{enumerate}
        \item $p(x+y) \leq p(x) + p(y)$
        \item $p(\lambda x) = \lambda p(x)$ where $\lambda > 0$.
    \end{enumerate}  
\end{definition}

\begin{theorem}[Hahn-Banach Real Version]
    Let $X$ be a $\R$-vector space, $p$ be a sublinear functional, $M$ be a subspace of $X$ and $f : M \to \R$ be a $\R$-linear functional satisfying $f \leq p$, then there exists a $\R$-linear extension $F : X \to \R$ with $F \leq p$ and $F|_M = f$.
\end{theorem}

\begin{proof}
    First, let $x \in X \setminus M$ and notice that for all $y_1, y_2 \in M$, we have
    $$f(y_1) + f(y_2) = f(y_1 + y_2) \leq p(y_1 + y_2) \leq p(y_1 - x) + p(x + y_2)$$
    which implies that
    $$f(y_1) - p(y_1 - x) \leq p(x + y_2) - f(y_2).$$
    Since it holds for all $y_1, y_2 \in M$, we get that
    $$\sup_{y \in M}\{f(y) - p(y - x)\} \leq \inf_{y \in M}\{p(x + y) - f(y)\}.$$
    From this, define $\alpha$ to be a real number satisfying
    $$\sup_{y \in M}\{f(y) - p(y - x)\} \leq \alpha \leq \inf_{y \in M}\{p(x + y) - f(y)\},$$
    and consider the function $g : M + \R x \to \R$ defined by $g(y + \lambda x) = f(y) + \lambda \alpha$. Since $x \notin M$, then $g$ is well-defined. Moreover, it is clear that $g$ is linear and that it extends $f$ (by letting $\lambda = 0$).

    Let's prove that $g \leq p$ by cases on the sign of $\lambda$. When $\lambda = 0$, it follows from the fact that $f \leq p$. When $\lambda > 0$, we can use the fact that
    $$\alpha \leq \inf_{y \in M}\{p(x + y) - f(y)\} \leq p\left(x + \frac{y}{\lambda}\right) - f\left(\frac{y}{\lambda}\right)$$
    to get that
    \begin{align*}
        g(y + \lambda x) &= \lambda \left[f\left(\frac{y}{\lambda}\right) + \alpha\right] \\
        &\leq \lambda \left[f\left(\frac{y}{\lambda}\right) + p\left(x + \frac{y}{\lambda}\right) - f\left(\frac{y}{\lambda}\right)\right] \\
        &= p(y + \lambda x).
    \end{align*}
    When $\lambda < 0$, we can use the fact that
    $$f\left(\frac{y}{-\lambda}\right) - p\left(\frac{y}{-\lambda} - x\right) \leq \sup_{y \in M}\{f(y) - p(y - x)\} \leq \alpha$$
    to get that
    \begin{align*}
        g(y + \lambda x) &= -\lambda \left[f\left(\frac{y}{-\lambda}\right) - \alpha\right] \\
        &\leq -\lambda \left[f\left(\frac{y}{-\lambda}\right) - f\left(\frac{y}{-\lambda}\right) + p\left(\frac{y}{-\lambda} - x\right) \right] \\
        &= p(y + \lambda x).
    \end{align*}
    Therefore, for all $y + \lambda x \in M + \R x$, we have that $g \leq p$. \\
    The goal now is to repeat this process infinitely many times. To do this rigorously, we will use Zorn's Lemma. Let $\mathcal{F}$ be the set of all linear extensions $F$ of $f$ such that $F \leq p$ and $F$ is defined on a subspace of $X$. View each elements of $\mathcal{F}$ as subsets of $X \times \R$ instead of linear functionals, then it follows that $\mathcal{F}$ is partially ordered by inclusion. Moreover, any chain has a maximal elements (which can be obtained by taking the union of the chain). It follows by Zorn's Lemma that $\mathcal{F}$ has a maximal element $F$. Hence, there exists a linear functional $F$ on a subspace $U$ of $X$ that extends $f$ and that satisfies $F \leq p$ such that no linear functional with the same properties extends $F$ and is defined on a larger subspace. However, if $U$ is different than $X$, then we can create such an extension of $F$ on $U + \R x_0$ (for some $x_0 \in X\setminus U$) as we did in the first part of this proof. But this would contradict the fact that $F$ is maximal. Therefore, $F$ is defined on $V$.
\end{proof}

\begin{theorem}[Hahn-Banach Complex Version]
    Let $X$ be a $\C$-vector space, $p$ be a semi-norm, $M$ be a subspace of $X$ and $f : M \to \C$ be a $\C$-linear functional satisfying $|f| \leq p$, then there exists a $\C$-linear extension $F : X \to \C$ with $|F| \leq p$ and $F|_M = f$.
\end{theorem}

\begin{proof}
    Let $u$ be the real part of $f$, then by the Real Version of Hahn-Banach, there exists a $\R$-linear functional $U$ that extends $u$ such that $U \leq p$. But notice that for all $x \in X$, since $U \leq p$ holds on $X$, then in particular, $U(x) \leq p(x)$ and $U(-x) \leq p(-x)$. Moreover, since $p$ is a semi-norm and $U$ is linear, then $U(-x) \leq p(-x)$ is equivalent to $-p(x) \leq U(x)$. Combining these inequalities, we get that $|U| \leq p$ holds on $X$. Thus, by a Proposition we proved earlier, we can construct from $U$ a $\C$-linear functional $F$ by
    $$F(x) = U(x) - iU(ix).$$
    when $x \in M$, we get that
    $$F(x) = U(x) - iU(ix) = u(x) - iu(ix) = f(x)$$
    so $F$ extends $f$. Moreover, for all $x \in X$, if $|F(x)| = 0$, then $|F(x)| \leq p(x)$, and if $|F(x)| \neq 0$, we can define $\alpha = \frac{\overline{F(x)}}{|F(x)|}$ to get that 
    $$|F(x)| = \alpha F(x) = F(\alpha x) \in \R$$
    which implies that
    $$|F(x)| = F(\alpha x) = U(\alpha x) \leq p(\alpha x) = |\alpha| p(x) = p(x).$$
    Therefore, $F$ is a $\C$-linear functional that extends $f$ and that satisfies $|F| \leq p$.
\end{proof}

\begin{theorem}
    Let $X$ be a NLS.
    \begin{enumerate}[label=(\alph*)]
        \item Suppose $M$ is closed subspace of $X$ and let $x \in X \setminus M$, then there exists a $f \in X^*$ such that $f(x) = \inf_{y\in M}\norm{x - y} > 0$, $f|_M = 0$ and $\norm{f} = 1$.
        \item If $x \neq 0$, there exists a $f \in X^*$ such that $\norm{f} = 1$ and $f(x) = \norm{x}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Define $\delta = \inf_{y\in M}\norm{x - y}$, if $\delta = 0$, then $x$ is a limit-point of $M$ which implies that $x \in M$ (using the fact that $M$ is closed). But this is a contradiction with the fact that $x \in X\setminus M$. Thus, $\delta > 0$. Define $g : M + \C x \to \C$ by $g(y+\lambda x) = \lambda \delta$. Clearly, $g$ is a $\C$-linear functional on $M + \lambda x$. Moreover, notice that for all $y + \lambda x \in M + \C x$, we have
        $$|g(y + \lambda x)| = |\lambda|\inf_{y\in M}\norm{x - y} \leq |\lambda| \norm{x - \left(-\frac{y}{\lambda}\right)} = \norm{y + \lambda x}.$$
        Thus, we can apply the Complex Version of the Hahn-Banach Theorem with $p = \norm{\cdot}$ to get that there exists a linear function $f : X \to \C$ such that $f$ extends $g$ and $|f(z)| \leq \norm{z}$ for all $z \in X$. It follows that for all $y \in M$, $f(y) = g(y) = 0$ so $f|_M = 0$. Moreover, $f(x) = g(x) = \delta$. Finally, let's prove that $\norm{f} = 1$. First,
        $$\norm{f} = \sup_{z \neq 0}\frac{\abs{fz}}{\norm{z}} \leq \sup_{z \neq 0}\frac{\norm{z}}{\norm{z}} = 1.$$
        To prove the reverse inequality, let $n \in \N$ and notice that by properties of the infimum, there exists a $y_n \in M$ such that $\norm{x - y_n} \leq \delta + \frac{1}{n}$. Hence, if we let $z_0 = x - y_n$, we get that
        $$\frac{|f(z_0)|}{\norm{z_0}} = \frac{|f(x) - f(y_n)|}{\norm{x - y_n}} \geq \frac{\delta}{\delta + \frac{1}{n}}$$
        which implies that
        $$\frac{\delta}{\delta + \frac{1}{n}} \leq \frac{|f(z_0)|}{\norm{z_0}} \leq \sup_{z \neq 0}\frac{|f(z)|}{\norm{z}} = \norm{f}.$$
        By taking $n \rightarrow \infty$, we obtain $1 \leq \norm{f}$. It follows that $\norm{f} = 1$.
        \item It follows from part (a) by taking $M = \{0\}$.
    \end{enumerate}
\end{proof}

\section{Consequences of the Baire Category Theorem}

\subsection{Baire Category Theorem}

\begin{definition}[Meager Sets]
    A topological space is said to be meager if it is a countable union of nowhere-dense sets. We can also say that it is of first category.
\end{definition}

\begin{theorem}[Baire Category Theorem]
    Let $X$ be a complete metric space.
    \begin{enumerate}[label=(\alph*)]
        \item If $\{U_n\}$ is a countable collection of open dense sets in $X$, then $\overline{\cap_{n=1}^{\infty}U_n} = X$.
        \item $X$ is not meager.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\subsection{Open Mapping Theorem}

\begin{definition}[Open Mapping]
    Let $X$ and $Y$ be topological spaces. We say that $f : X \to Y$ is open if $f(U)$ is open in $Y$ for all open subsets $U$ of $X$.
\end{definition}

\begin{proposition}[Characterization of Open Mappings \MakeUppercase{\romannumeral 1}]
    Given two metric spaces $X$ and $Y$ and a function $f : X \to Y$, we have that $f$ is open if and only if for all balls $B(x, r_1) \subset X$, there is a $r_2 > 0$ such that $B(f(x), r_2) \subset f(B(x,r))$.
\end{proposition}

\begin{proof}
    \td 
\end{proof}

\begin{proposition}[Characterization of Open Mappings \MakeUppercase{\romannumeral 2}]
    Given two NLS $X$ and $Y$ and $f : X \to Y$ a linear map, we have that $f$ is open if and only if $f(B(0,1))$ contains a ball around 0.
\end{proposition}

\begin{proof}
    \td 
\end{proof}

\begin{theorem}[Open Mapping Theorem]
    Let $X$ and $Y$ be Banach and $T \in \L(X,Y)$. If $T$ is surjective, then $T$ is open.
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\begin{corollary}
    Suppose $X$ and $Y$ are Banach and $T \in \L(X,Y)$ is bijective, then $T^{-1} \in \L(Y,X)$. 
\end{corollary}

\begin{proof}
    \td 
\end{proof}

\begin{corollary}
    Let $X$ be a vector space and $\lnorm{1}{\cdot}$, $\lnorm{2}{\cdot}$ be norms on $X$ such that both $(X, \lnorm{1}{\cdot})$ and $(X, \lnorm{2}{\cdot})$ are complete. If there is a contant $C > 0$ such that $\lnorm{2}{x} \leq C \lnorm{1}{x}$ for all $x \in X$, then there is a constant $C' > 0$ such that $\lnorm{1}{x} \leq C' \lnorm{2}{x}$ for all $x \in X$.
\end{corollary}

\begin{proof}
    \td 
\end{proof}

\subsection{Closed Graph Theorem}

\begin{definition}[Closed Mapping]
    Given two NLS $X$ and $Y$ and a linear map $T : X \to Y$, we say that $T$ is closed if its graph is a closed subset of $X \times Y$.
\end{definition}

\begin{theorem}[Closed Graph Theorem]
    Let $X$ and $Y$ be Banach and $T:X \to Y$ be a closed linear map, then $T$ is bounded.  
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\subsection{Uniform Boundedness Principle}

\begin{theorem}[Uniform Boundedness Principle]
    Let $X$ and $Y$ be NLS and $A \subset \L(X,Y)$.
    \begin{enumerate}[label=(\alph*)]
        \item If $\sup_{T \in A}\norm{Tx} < \infty$ for all $x \in B$ where $B$ is non-meager, then $\sup_{T\in A}\norm{T} < \infty$.
        \item If $X$ is Banach and $\sup_{T \in A}\norm{Tx} < \infty$ for all $x \in X$, then $\sup_{T\in A}\norm{T} < \infty$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\section{Hilbert Spaces}

\subsection{Definitions and Basic Properties}

\begin{definition}[Inner Product]
    Let $\H$ be a $\C$-vector space. An inner-product on $\H$ is a mapping $\inner{\cdot}{\cdot} : \H \times \H \to \C$ that satisfies
    \begin{enumerate}
        \item $\inner{ax+by}{z} = a\inner{x}{z} + b\inner{y}{z}$ for all $a,b \in \C$ and $x,y,z \in \H$.
        \item $\inner{y}{x} = \overline{\inner{x}{y}}$ for all $x,y \in \H$.
        \item $\inner{x}{x} > 0$ for all $x \in \H$.
    \end{enumerate}
    In that case, we call $\H$ an Inner Product Space (IPS).
\end{definition}

\begin{theorem}[Cauchy-Schwarz]
    Let $(\H, \inner{\cdot}{\cdot})$ be an IPS, then
    \begin{enumerate}[label=(\alph*)]
        \item $|\inner{x}{y}| \leq \sqrt{\inner{x}{x}} \sqrt{\inner{y}{y}}$ for all $x,y \in \H$.
        \item The two sides of the previous inequality are equal precisely when $x = \alpha y$ for some $\alpha \in \C$.
    \end{enumerate}
\end{theorem}

\begin{proposition}
    Inner Product Spaces are Normed Linear Spaces.
\end{proposition}

\begin{proof}
    $\norm{x} = \sqrt{\inner{x}{x}}$.\td
\end{proof}

\begin{definition}[Hilbert Space]
    If an Inner Product Space is a Banach Space (as a NLS), then it is a Hilbert Space.
\end{definition}

\subsection{Orthogonal Decomposition}

\begin{proposition}[Continuity of the Inner-Product]
    Let $\H$ be an IPS and $\{x_n\}_n$ and $\{y_n\}_n$ be sequences in $\H$ such that $x_n \rightarrow x$ and $y_n \rightarrow y$ for some $x,y \in \H$, then $\inner{x_n}{y_n} \rightarrow \inner{x}{y}$.
\end{proposition}

\begin{proof}
    \td 
\end{proof}

\begin{corollary}[Continuity of the Norm]
    Let $\H$ be an IPS, then the norm induced by the inner-product is continuous.
\end{corollary}

\begin{proof}
    \td 
\end{proof}

\begin{theorem}[Parallelogram Law]
    For any $x,y \in \H$,
    $$\norm{x+y}^2 + \norm{x - y}^2 = 2(\norm{x}^2 + \norm{y}^2)$$
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\begin{definition}[Orthogonality]
    Given $x,y \in \H$, we say that $x$ and $y$ are orthogonal and write $x \perp y$ when $\inner{x}{y} = 0$. Given $E \subset H$, we define
    $$E^{\perp} = \{y \in \H : \inner{x}{y} = 0 \ \forall x \in \H\}.$$
\end{definition}

\begin{theorem}[Pythagorean Theorem]
    Given a finitely many pairwise orthogonal vectors $x_1, ..., x_n$ in $\H$, the following equation holds
    $$\norm{\sum_{i=1}^{n}x_i}^2 = \sum_{i=1}^{n}\norm{x_i}^2.$$
\end{theorem}

\begin{proof}
    \td
\end{proof}

\begin{theorem}[Orthogonal Decomposition]
    Let $\H$ be a Hilbert Space and $M$ be a closed subspace $\H$, then $\H = M \oplus M^{\perp}$.
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\subsection{Riesz Representation Theorem}

\begin{theorem}[Riesz Representation Theorem]
    If $f \in \H^*$, then there exists a unique $y \in \H$ such that
    $$f(x) = \inner{x}{y}$$
    for all $x \in \H$.
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\subsection{Hilbert Bases}

\subsection{Fourier Series}

\section{$L^p$ Spaces}

\subsection{Definitions and Basic Properties}

\begin{definition}
    Given a measure space $(X, \M, \mu)$ and $p > 0$, we define the norm $\lnorm{p}{\cdot}$ on the $\text{Meas}(X)$ by
    $$\lnorm{p}{f} = \left(\int_X |f|^p d\mu \right)^{1/p}$$
\end{definition}

\begin{definition}
    Given a measure space $(X, \M, \mu)$ and $p > 0$, we define the space $L^p(X, \M, \mu)$ by
    $$L^p(X, \M, \mu) = \{f : X \to \C : f\in \text{Meas}(X) \text{ and } \lnorm{p}{f} < \infty\}.$$
\end{definition}

\begin{proposition}
    Given a measure space $(X, \M, \mu)$ and $p > 0$, the space $L^p(X, \M, \mu)$ is a vector space.
\end{proposition}

\begin{proof}
    \td 
\end{proof}


\subsection{Hölder and Minkowski}

\begin{lemma}[Convexity Bound]
    Suppose $b > a > 0$, $0 < p < 1$ and $0 < \lambda < 1$, then
    $$a^{\lambda} b^{1 - \lambda} \leq \lambda a + (1 - \lambda) b. $$
\end{lemma}

\begin{proof}
    \td
\end{proof}

Notice that when $\lambda = 1/2$, this is the GM-AM inequality. 

\begin{theorem}[Hölder's Inequality]
    Let $1 < p < \infty$, $q$ such that $\frac{1}{p} + \frac{1}{q} = 1$ and $f$ and $g$ be measurable functions, then
    $$\lnorm{1}{fg} \leq \lnorm{p}{f} \lnorm{q}{g}.$$
    Equality holds if and only if $\alpha |f|^p = \beta |g|^q$ a.e for some complex numbers $\alpha$ and $\beta$.
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\begin{theorem}[Minkowski's Inequality]
    Let $(X, \M, \mu)$ be a measurable space, $1 \leq p < \infty$ and $f,g \in \L^p(X, \M, \mu)$, then
    $$\lnorm{p}{f+g} \leq \lnorm{p}{f} + \lnorm{p}{g}.$$
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\begin{proposition}
    Given a measure space $(X, \M, \mu)$ and $1 \leq p < \infty$, the space $L^p(X, \M, \mu)$ is a NLS.
\end{proposition}

\begin{proof}
    \td 
\end{proof}

\begin{proposition}
    Given a measure space $(X, \M, \mu)$ and $1 \leq p < \infty$, the space $L^p(X, \M, \mu)$ is Banach.
\end{proposition}

\begin{proof}
    \td 
\end{proof}

\begin{proposition}
    Let $(X, \M, \mu)$ be a measure space and $1 \leq p < \infty$, then the set
    $$\text{Sim}(X) = \{f \in \text{Meas}(X) : \Im(f) \text{ is finite and } \mu(f^{-1}x) < \infty \ \forall x \in X\}.$$
    is dense in $L^p(X, \M, \mu)$.
\end{proposition}

\begin{proof}
    \td 
\end{proof}

\subsection{The case $p = \infty$}

\begin{definition}[Sup-Norm]
    Let $(X, \M, \mu)$ be a measure space and $f \in \text{Meas}(X)$, then we define 
    $$\lnorm{\infty}{f} = \text{essup}_{x \in X}|f(x)| = \inf \{a \geq 0 : \mu(|f|>a) = 0\}$$
    and call it the sup-norm of $f$.
\end{definition}

\begin{definition}
    Given a measure space $(X, \M, \mu)$, we define the space $L^{\infty}(X, \M, \mu)$ by
    $$L^{\infty}(X, \M, \mu) = \{f : X \to \C : f\in \text{Meas}(X) \text{ and } \lnorm{\infty}{f} < \infty\}.$$
\end{definition}

\begin{theorem}[Properties of $L^{\infty}$]
    Let $(X, \M, \mu)$ be a measure space.
    \begin{enumerate}
        \item Given $f,g \in \text{Meas}(X)$, then 
        $$\lnorm{1}{fg} \leq \lnorm{1}{f} \lnorm{\infty}{g}.$$
        and equality holds if and only if $|g(x)| = \lnorm{\infty}{g}$ a.e on the set $\{f \neq 0\}$.
        \item The sup-norm is indeed a norm on $L^{\infty}$.
        \item Given a sequence $\{f_n\}_n$ and a function $f$, we have that $\lnorm{\infty}{f_n - f} \rightarrow 0$ if and only there is a co-null set $E \in \M$ such that $f_n \rightarrow f$ uniformly on $E$.
        \item $L^{\infty}$ is Banach.
        \item The simple functions are dense in $L^{\infty}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \td 
\end{proof}

\subsection{Useful Inequalities}

\subsection{Duality in $L^p$}

\section{Spectral Theory}

Motivation: \td (Linear Algebra) Given a linear operator $A : \C^n \to \C^n$ such that $A^* = A$ (where $A^*$ is the conjugate transpose operator), we have that there exists a unitary matrix $U$ (unitary = $U^* U = U U^* = I$) such that $UAU^{-1}$ is diagonal. How to generalize this to Hilbert spaces ? What is the spectrum of a self-adjoint operator ? In finite dimensions, we can consider the resolvent set of $A$ defined by $\rho(A) = \C \setminus \sigma(A)$. In finite dimensions, if $\lambda \in \rho(A)$, then $(A - \lambda I)$ is an isomorphism. \\
From this point, assume that $\H$ is a separable Hilbert spaces over $\C$.

\begin{definition}[Resolvent Set]
    Given a Hilbert Space and $A \in \L(\H)$, the resolvent set of $A$ is defined by
    $$\rho(A) = \{\lambda \in \C : (A - \lambda I) \text{ is an isomorphism}\}$$
\end{definition}

\begin{definition}[Spectrum]
    Given a Hilbert Space and $A \in \L(\H)$, the spectrum of $A$ is defined as the complement of the resolvent set of $A$:
    $$\sigma(A) = \C \setminus \rho(A).$$
\end{definition}

\begin{proposition}
    Suppose $A \in \L(\H)$ and $\lambda \in \rho(A)$, then both $(A - \lambda I)$ and $(A - \lambda I)^{-1}$ are bijective bounded linear maps.
\end{proposition}

\begin{proof}
    Use the Corollary of the Open Mapping Theorem (Inverse Mapping Theory).\td 
\end{proof}

\begin{definition}[Resolvent Operator]
    Given $\lambda \in \rho(A)$, we define $R_{\lambda}(A) = (A - \lambda I)^{-1}$ and call it the resolvent operator of $A$. 
\end{definition}

The spectrum of $A$ can be decomposed as follows:

\begin{definition}
    Let $A \in \L(\H)$.
    \begin{enumerate}
        \item (Point Spectrum) $$\sigma_p(A) = \{\lambda \in \sigma(A) : (A - \lambda I) \text{ is not 1-1}\}.$$
        \item (Continuous Spectrum) $$\sigma_c(A) = \{\lambda \in \sigma(A) : (A - \lambda I) \text{ is 1-1 but is not onto and } \overline{\range(A - \lambda I)} = \H\}.$$
        \item (Residual Spectrum)
        $$\sigma_r(A) = \{\lambda \in \sigma(A) : (A - \lambda I) \text{ is 1-1 and } \overline{\range(A - \lambda I)} \neq \H\}.$$
    \end{enumerate}
\end{definition}

\td Newmann Series : Suppose $B \in \L(\H)$ with $\norm{B} < 1$. The Newmann Series of $B$ is a operated-valued Taylor series that converges in norm to $(I - B)^{-1}$. The motivation comes from the geometric series that gives us $\sum_{n=0}^{\infty}x^n = 1/(1 - x)$ for $|x| < 1$. When $\norm{B} < 1$, we can prove in a similar way that
$$\norm{(I - B)^{-1} - \sum_{n=0}^{N}B^n} \rightarrow 0$$
as $N \rightarrow \infty$ by noting that
$$\norm{I - (I - B)\sum_{n=0}^{N}B^n} \leq \norm{B}^{N+1}.$$

\begin{definition}
    An operator-valued function $F : \Omega \to \L(\H)$ with $\Omega \subset \C$ open is analytic at $x_0 \in \Omega$ if there exist $\{B_n\} \subset \L(\H)$ and $\delta > 0$ such that 
    $$F(z) = \sum_{n=0}^{\infty}(z - z_0)^n B_n$$
    in the disk $D(\delta) = \{z \in \C : |z - z_0| < \delta\}$ where the series converges in the operator norm.
\end{definition}

\begin{proposition}
    Suppose $A \in \td \B(\H)$, then
    \begin{enumerate}
        \item $\rho(A) \subset \C$ is open.
        \item $\{\lambda \in \C : \norm{A} > |\lambda|\} \subset \rho(A)$.
        \item The function $\lambda \mapsto R_{\lambda}(A)$ is analytic.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item Suppose $\lambda_0 \in \rho(A)$, then we can write 
        $$(\lambda I - A) = (\lambda_0 I - A)[1 - (\lambda_0 - \lambda)(\lambda_0 I - A)^{-1}].$$
        If $|\lambda_0 - \lambda| < \norm{\lambda_0 I - 1}$, then we can expand the right hand side as a Newmann series. This gives $(\lambda I - A)^{-1}$ as an operator-valued analytic function where $|\lambda_0 - \lambda| < \norm{\lambda_0 I - 1}$. Here, we get that $R_{\lambda}(A)$ is analytic when $\lambda \in \rho(A)$. \td
        \item If $\lambda \in \rho(A)$ and $\lambda \neq 0$, then $R_{\lambda}(A) = \lambda^{-1}(I - \lambda^{-1}A)^{-1}$. But again, by the Newmann series, if $\norm{A} < |\lambda|$, we get that $(I - \lambda^{-1}A)^{-1}$ is an analytic operator-valued function. This implies that $R_{\lambda}(A)$ is analytic when $|\lambda| > \norm{A}$. This implies that $\sigma(A)$ is contained inside the the closed disk centered at 0 of radius $\norm{A}$. \td ??
    \end{enumerate}
\end{proof}

\begin{example}
    \item Take $\H = L^2([0,1])$ with the Lebesgue measure. Define $M : \H \to \H$ by $f(x) \mapsto x\cdot f(x)$. How to compute the spectrum of $M$ ? First, (Ex), $M \in \B(\H)$ with a norm of 1. \td To calculate the point spectrum of $M$, notice that
    $$Mf = \lambda f \implies (x - \lambda)f = 0 \implies f = a.e \implies \sigma_p(A) = \varnothing$$
    For the continuous spectrum, if $\lambda \in [0,1]$, then $M-\lambda I$ is not onto $L^2$ since constant functions are not in the range. However, the range of $M - \lambda I$ is dense in $L^2$ since for any $f \in L^2$, we can set
    $$f_n(x) = \begin{cases}
        f(x); & |x - \lambda| > \frac{1}{n} \\ 0 & |x - \lambda| \leq \frac{1}{n}.
    \end{cases}$$
    then $\lnorm{2}{f_n - f} \rightarrow 0$. If $f \in L^2$ and $\lambda \notin [0,1]$, then $(x - \lambda)^{-1}f \in L^2$ so the spectrum of $A$ is precisely [0,1]. \td 
\end{example}

\begin{definition}[Spectral Radius]
    Given a bounded operator $A \in \B(\H)$, we define
    $$r(A) = \sup\{|\lambda| : \lambda \in \sigma(A)\}$$
    and call it the spectral radius of $A$.
\end{definition}

\begin{remark}
    From the previous proposition, we have that $r(A) \leq \norm{A}$.
\end{remark}

\begin{definition}
    Given a bounded operator $A \in \B(\H)$, we define $A^*$ to be the operator the satisfies
    $$\inner{Ax}{y} = \inner{x}{A^* y}$$
    for all $x,y \in \H$. It exists by defining it on the basis. 
\end{definition}

\begin{theorem}
    Let $A \in \B(\H)$, then
    \begin{enumerate}[label=(\alph*)]
        \item $r(A) = \lim_{n \rightarrow \infty} \norm{A^n}^{1/n}$.
        \item If $A^* = A$, then $r(A) = \norm{A}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Let's first prove that $\lim_{n \rightarrow \infty}\frac{1}{n}\log\norm{A^n}$ exists. Set $a_n = \log \norm{A^n}$, then by properties of the logarithm, we have that $a_{n+1} \leq a_n + a_1$. Similarly, for all $m,n \in \Z$ with $m < n$, we can write $n = pm + q$ with $0 \leq q < m$. It follows that $a_n \leq a_{pm} + a_q \leq pa_m + a_q$. Dividing by $n$ on both sides gives us 
        \[\frac{a_n}{n} \leq \frac{p}{n}a_m + \frac{a_q}{n} \tag*{(1)}.\]
        Fix $m$, then the equation $n = pm + q$ can be rewritten as
        $$1 = \frac{pm}{n} + \frac{q}{n}.$$
        If we let $n$ go to infinity, then we obtain
        $$\lim_{n \rightarrow \infty}\frac{p}{n} = \frac{1}{m}$$
        since $q$ is bounded by $n$. It follows that equation (1) implies \td  $\limsup \frac{a_n}{n} \leq \liminf \frac{a_m}{m}$. Therefore, $\lim \frac{a_n}{n}$ exists. By the Root Test for numerical series, $\sum_{j=0}^{\infty}A^j$ converges if \td $r(A) < 1$ and diverges if $r(A) > 1$. If $r(A) < 1$, ($\lim \norm{A^n}^{1/n} < 1$), then there exist $R$ and $N$ such that $r(A) < R < 1$ and $\norm{A^n} \leq R^n$ when $n \geq N$. [...] \td 
        \item If $A^* = A$, it suffices to show that $\norm{A^2} = \norm{A}^2$ (Exercise \td). Then By induction, we get that $\norm{A^{2^m}} = \norm{A}^{2^m}$ for all natural numbers $m$. It follows that
        $$r(A) = \lim_{m\rightarrow\infty}\norm{A^m}^{1/m} = \lim_{m \rightarrow \infty}\norm{A^{2^m}}^{1/2^m} = \lim_{m \rightarrow \infty}(\norm{A}^{2^m})^{1/2^m} = \norm{A}.$$
    \end{enumerate}
\end{proof}

\begin{remark}
    Can $\sigma(A) = \varnothing$ ?
\end{remark}

\begin{theorem}
    Let $A \in \B(\H)$, then $\sigma(A) \neq \varnothing$.
\end{theorem}

\begin{remark}
    To prove this, we will use a little bit of Complex Analysis. We say that a complex function $f : \C \to \C$ is entire if we can write
    $$f(z) = \sum_{k=0}^{\infty}a_k z^k$$
    where the right hand side converges absolutely and uniformly for all $z \in \C$. For example, polynomials and the exponential function are entire functions. Liouville's Theorem states that a bounded entire function must be constant. 
\end{remark}

\begin{proof}
    To prove the theorem, recall that $R_{\lambda}$ is analytic. For any fixed $x,y \in \H$, define the function $f(\lambda) = \inner{R_{\lambda}x}{y}$. If $\sigma(A) = \varnothing$, then $\rho(A) = \C$ and so $f$ is entire. Hence, by Cauchy-Schwarz,
    $$|f(\lambda)| = |\inner{R_{\lambda} x}{y}| \leq \norm{R_{\lambda}x} \cdot \norm{y}.$$
    Moreover, notice that
    $$ \norm{R_{\lambda}x} = |\lambda|^{-1} \norm{\left(\frac{A}{\lambda} - I\right)^{-1} x} \leq |\lambda|^{-1} \norm{\left(\frac{A}{\lambda} - I\right)^{-1}}\norm{x}.$$
    It follows that when $\lambda \rightarrow \infty$, we have that $\norm{R_{\lambda}x} \rightarrow 0$. Therefore, $|f(\lambda)| \rightarrow 0$ as $\lambda \rightarrow \infty$. Hence, $f$ is bounded. This means by Liouville's Theorem that $f$ is constant with limit $0$ so $f$ must be identically 0 on $\C$. Therefore, $(A - \lambda I)^{-1} = 0$ which is a contradiction. 
\end{proof}

\subsection{Basic Results for $\sigma_p(A)$}

\begin{lemma}
    Suppose $A \in \B(\H)$ and $A^* = A$, then
    \begin{enumerate}[label=(\alph*)]
        \item $\sigma_p(A) \subset \R$.
        \item If $Av_i = \lambda_i v_i$ for a $\lambda_i \in \sigma_p(A)$ and $v_i \in \H$, and $Av_j = \lambda_j v_j$ for a $\lambda_i \in \sigma_p(A)$ and $v_i \in \H$ with $\lambda_i \neq \lambda_j$, then $\inner{v_i}{v_j} = 0$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Suppose $Ax = \lambda x$ for some $\lambda \in \sigma_p(A)$ and $x \in \H$, then 
        $$\lambda \norm{x}^2 = \inner{Ax}{x} = \inner{x}{Ax} = \overline{\lambda}\norm{x}^2$$
        which implies that $\lambda = \overline{\lambda}$ so $\lambda$ is real.
        \item Suppose $Ax = \lambda x$ and $A y = \mu y$ for some $x,y \in \H$ and $\lambda, \mu \in \sigma_p(A)$, then
    $$\lambda \inner{x}{y} = \inner{Ax}{y} = \inner{x}{Ay}  = \mu \inner{x}{y}$$
    so $\inner{x}{y} = 0$ since $\lambda \neq \mu$. 
    \end{enumerate}
\end{proof}

Eigenspaces are orthogonal to each other.

\subsection{Spectral Theory of Compact Operators}

\begin{definition}
    A closed subspace $M \leq \H$ is an invariant subspace of $A \in \B(\H)$ is $x \in M$ implies $Ax \in M$.
\end{definition}

\begin{proposition}
    Let $A \in \B(\H)$, suppose that $A^* = A$ and let $M \leq \H$ be invariant, then $M^{\perp}$ is also invariant.
\end{proposition}

\begin{proof}
    Suppose $x \in M^{\perp}$ and $y \in M$, then
    $$\inner{y}{Ax} = \inner{Ay}{x} = 0$$
    where the last inequality comes from the fact that $Ay \in M$. Since it holds for all $y \in M$, then $Ax \in M$. Therefore, $M^{\perp}$ is invariant.
\end{proof}

\begin{lemma}
    Suppose $A \in \B(\H)$ and $\lambda \in \C$, then 
    $$\overline{\Im(A - \lambda I)} = \ker(A^* - \overline{\lambda} I)^{\perp}.$$
\end{lemma}

\begin{proof}
    (Exercise) \td
\end{proof}

\begin{proposition}
    Suppose $\lambda \in \sigma_r(A)$ with $A \in \B(\H)$, then $\overline{\lambda} \in \sigma_r(A^*)$.
\end{proposition}

\begin{proof}
    If $\lambda \in \sigma_r(A)$, then $\overline{\Im(A - \lambda I)} \neq \H$. Since $\overline{\Im(A - \lambda I)}$ is closed, then we can find a $x \in \H \setminus \{0\}$ such that $x \perp \overline{\Im(A - \lambda I)}$ and hence $x \perp \Im(A - \lambda I)$. This implies that for any $y \in \H$, we have $\inner{x}{(A - \lambda I)y} = 0$. Equivalently, $\inner{(A^* - \overline{\lambda} I)x}{y} = 0$ for all $y \in \H$.
\end{proof}

\td \td \td \td

\subsection{Compact Operators}

\begin{definition}
    An operator $A \in \B(\H)$ is compact if it maps bounded sets to precompact ones, i.e., if given a bounded sequence $\{x_n\}$ in $\H$, then the sequence $\{Ax_n\}$ has a convergent subsequence.
\end{definition}

\td (a lot) \td 

\subsection{Spectral Theorem for Compact Operators}

\begin{theorem}[Spectral Theorem]
    Let $A \in \text{Com}(\H)$ be self-adjoint, then there exusts an orthonormal Hilbert basis $\{e_k\}_k$ of $\H$ consisting of eigenvectors of $A$. The non-zero eigenvalues $\{\lambda_k\}$ are at most countable and real with
    $$A = \sum_{k=1}^{\infty}\lambda_k P_k$$
    where $P_k : \H \to V_k$ is the orthogonal projection onto $V_k$ which is the eigenspace associated with $\lambda_k$ ($\dim V_k < \infty$). Finally, if there are infinitely many non-zero $\lambda_k$, then
    $$\lim_{N \rightarrow \infty}\norm{A - \sum_{k=1}^{N}\lambda_k P_k} = 0.$$
\end{theorem}

\begin{proof}
    Let's prove it by induction. First, let's show that $\lambda = \norm{A}$ or $\lambda = -\norm{A}$ is an eigenvalue. To see this, we assume that $A \neq 0$.
\end{proof}



\end{document}