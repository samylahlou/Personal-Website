\documentclass{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage[margin=41mm]{geometry}

%% Sets page size and margins
%\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{enumitem}

%Theorem
\newtheorem*{theorem}{Theorem}
\newtheorem*{proposition}{Proposition}
\newtheorem*{lemma}{Lemma}
\newtheorem*{definition}{Definition}

%Commands definitions
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\lnorm}[2]{\left\lVert#2 \right\rVert_{#1}}
\newcommand{\norm}[1]{\left\lVert#1 \right\rVert}
\newcommand{\abs}[1]{\left\lvert#1 \right\rvert}
\newcommand{\Iint}[2]{\llbracket #1 , #2 \rrbracket}
\renewcommand{\Im}{\text{Im}}
\renewcommand{\Re}{\text{Re}}
\newcommand{\card}{\text{card}}
\newcommand{\isomorphic}{\cong}
\newcommand{\td}{\textcolor{red}{\textbf{TODO}}}

%Example environment
\newenvironment{example}{\noindent\textbf{Example:} \vspace{-0.2cm}\begin{itemize}}{\end{itemize}}

%Set QED symbol to blacksquare
\renewcommand\qedsymbol{$\blacksquare$}


\title{MATH 565 Notes : Functional Analysis}
\author{Samy Lahlou}
\date{}

\begin{document}

\maketitle

These notes are based on lectures given by Professor John Toth at McGill University in Winter 2025. The subject of these lectures is Functional Analysis, the prerequisites are Real Analysis and Linear Algebra. \\
As a disclaimer, it is more than possible that I made some mistakes. Feel free to correct me or ask me anything about the content of this document at the following address : samy.lahloukamal@mcgill.ca

\tableofcontents

\newpage

\section{Normed Linear Spaces}

In these notes, consider that the field $\F$ is either $\R$ or $\C$ if it's not specified.

\begin{definition}[Semi-norm]
    Given a vector space $X$ over $\F$, we call the function $\|\cdot\| : X \to [0, \infty)$ a semi-norm if
    \begin{enumerate}
        \item $\|x + y\| \leq \|x\| + \|y\|$ for all $x,y\in X$.
        \item $\|\lambda x\| = |\lambda| \|x\|$ for all $\lambda \in \F$ and $x \in X$.
    \end{enumerate}
    The second condition is called the \textit{Triangle Inequality}.
\end{definition}

\begin{definition}[Norm \& Normed Linear Space]
    Given a vector space $X$ over $\F$, we call the function $\|\cdot\| : X \to [0, \infty)$ a norm if it is a semi-norm that satisfies the following additional condition: 
    \begin{enumerate}
        \item $\|x\| = 0 \implies x = 0$ for all $x\in X$.
    \end{enumerate}
    In that case, we call $(X, \|\cdot\|)$ a Normed Linear Space (NLS).
\end{definition}

\begin{example}
    \item All finite-dimesnional vector spaces $V$ are NLS using the isomorphism $V \isomorphic \F^n$ where $n = \dim V$.
    \item Given a measure space $(X, \M, \mu)$, we can define the norm $\lnorm{1}{\cdot}$ on $L^1(\mu)$ by
    $$\lnorm{1}{f} = \int_X |f| d\mu.$$
    It is a semi-norm because the integral is linear and because the triangle inequality holds in $\F$ with $|\cdot|$. It is a norm because
    \begin{align*}
        \lnorm{1}{f} = 0 &\implies \int_X |f| d\mu = 0 \\
        &\implies |f| = 0 \ \ \mu.a.e \\
        &\implies f = 0 \ \ \mu.a.e \\
        &\implies f = 0 \ \ \text{in } L^1(\mu)
    \end{align*}
    It follows that $L^1(\mu)$ is a NLS.
\end{example}

\begin{definition}[Equivalent Norms]
    Given a vector space $X$ and two norms $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ on $X$, we say that the two norms are equivalent if there exist constants $C_1, C_2 > 0$ such that 
    $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
    for all $x \in X$.
\end{definition}

\begin{proposition}
    Given a vector space $X$, define the relation $\sim$ on the norms on $X$ as follows: $\lnorm{1}{\cdot} \sim \lnorm{2}{\cdot}$ if and only if there exist constants $C_1, C_2 > 0$ such that 
    $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
    for all $x \in X$, then $\sim$ is an equivalence relation.
\end{proposition}

\begin{proof} \leavevmode
    \begin{itemize}
        \item (Reflexive) Let $\norm{\cdot}$ be a norm on $X$, then $\norm{\cdot} \sim \norm{\cdot}$ by setting $C_1, C_2 > 0$.
        \item (Symmetric) Let $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ be norms on $X$ such that $\lnorm{1}{\cdot} \sim \lnorm{2}{\cdot}$, then there exist constants $C_1, C_2 > 0$ such that 
        $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
        for all $x \in X$. Notice that for all $x \in X$, we have
        \[C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \implies \lnorm{1}{x} \leq \frac{1}{C_1}\lnorm{2}{x} \tag*{(1)} \]
        and 
        \[\lnorm{2}{x} \leq C_2 \lnorm{1}{x} \implies \frac{1}{C_2}\lnorm{2}{x} \leq \lnorm{1}{x}.\tag*{(2)} \]
        Thus, combining (1) and (2) gives us
        $$\frac{1}{C_2}\lnorm{2}{x} \leq \lnorm{1}{x} \leq \frac{1}{C_1}\lnorm{2}{x}$$
        for all $x \in X$. Therefore, $\lnorm{2}{\cdot} \sim \lnorm{1}{\cdot}$.
        \item (Transitive) Let $\lnorm{1}{\cdot}, \lnorm{2}{\cdot}, \lnorm{3}{\cdot}$ be norms on $X$ such that $\lnorm{1}{\cdot} \sim \lnorm{2}{\cdot}$ and $\lnorm{2}{\cdot} \sim \lnorm{3}{\cdot}$, then there exist constants $C_1, C_2, C_3, C_4 > 0$ satisfying
        $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
        and 
        $$C_3 \lnorm{2}{x} \leq \lnorm{3}{x} \leq C_4 \lnorm{2}{x}$$
        for all $x \in X$. It follows that
        $$C_3 C_1 \lnorm{1}{x} \leq C_3 \lnorm{2}{x} \leq \lnorm{3}{x} \leq C_4 \lnorm{2}{x} \leq C_4 C_2 \lnorm{1}{x}$$
        for all $x \in X$. Therefore, $\lnorm{1}{\cdot} \sim \lnorm{3}{\cdot}$
    \end{itemize}
\end{proof}

\begin{proposition}
    Given a NLS $(X, \norm{\cdot})$, the function $d : X^2 \to [0, \infty)$ defined by $d(x,y) = \norm{x - y}$ is a metric on $X$.
\end{proposition}

\begin{proof}
    First, let's show that $d(x,y) = 0$ if and only if $x = y$. To do so, suppose that $d(x,y) = 0$, then by definition, $\norm{x - y} = 0$ which directly implies $x - y = 0$, which is equivalent to $x = y$. Suppose now that $x = y$, then 
    $$d(x,y) = \norm{x - y} = \norm{0} = 0.$$
    Take $x,y \in X$, and notice that
    \begin{align*}
        d(x,y) &= \norm{ x- y}\\
        &= \norm{(-1)(y - x)} \\
        & = |-1|\norm{y - x} \\
        &= d(y,x).
    \end{align*}
    Finally, for any $x,y,z \in X$, we have
    \begin{align*}
        d(x,y) &= \norm{x - y} \\
        &= \norm{x - z + z - y} \\
        &\leq \norm{x - z} + \norm{z - y} \\
        &= d(x,z) + d(z,y).
    \end{align*}
    Therefore, $d$ is a metric on $X$.
\end{proof}

\begin{definition}[Induced Topology]
    A NLS $(X, \norm{\cdot})$ induces a metric space $(X, d)$ where the metric $d : X \times X \to [0, \infty)$ is defined by $d(x,y) = \norm{x - y}$. This metric space induces a topological space $(X, \tau)$ where $\tau$ is generated by the open balls of $(X, d)$. We call $\tau$ the topology induced by the norm $\norm{\cdot}$.
\end{definition}

\begin{proposition}
    Let $X$ be a vector space and $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ be two norms on $X$. If the norms are equivalent, then they induce the same topology on $X$.
\end{proposition}

\begin{proof}
    Since $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ are equivalent, then there exist constants $C_1, C_2 > 0$ such that
    $$C_1 \lnorm{1}{x} \leq \lnorm{2}{x} \leq C_2 \lnorm{1}{x}$$
    for all $x \in X$. Let $T_1$ and $T_2$ be the sets of open balls in the metric spaces induced by $\lnorm{1}{\cdot}$ and $\lnorm{2}{\cdot}$ respectively:
    $$T_1 = \{B_{\epsilon}^{(1)}(x) : \epsilon > 0 \text{ and } x\in X\}$$
    $$T_2 = \{B_{\epsilon}^{(2)}(x) : \epsilon > 0 \text{ and } x\in X\}$$
    where 
    $$B_{\epsilon}^{(1)}(x) = \{y \in X : \lnorm{1}{x - y} < \epsilon\}$$
    and 
    $$B_{\epsilon}^{(2)}(x) = \{y \in X : \lnorm{2}{x - y} < \epsilon\}.$$
    Moreover, we know that the topology induced by $\lnorm{1}{\cdot}$ is $\tau_1 = \langle T_1 \rangle$ and the topology induced by $\lnorm{2}{\cdot}$ is $\tau_2 = \langle T_2 \rangle$. Let's show that $\tau_1 = \tau_2$. To do this, let $\epsilon > 0$ and $x \in X$ be arbitrary, let's show that $B_{\epsilon}^{(1)}(x)$ is open in the topology $\tau_2$. For all $z \in B_{\epsilon}^{(1)}(x)$, there exists a $\delta_z > 0$ such that $B_{\delta_z}^{(1)}(z) \subset B_{\epsilon}^{(1)}(x)$. Let's prove that
    $$B_{\epsilon}^{(1)}(x) = \bigcup_{z \in  B_{\epsilon}^{(1)}(x)}B_{C_1\delta_z}^{(2)}(z).$$
    First, if $y \in B_{\epsilon}^{(1)}(x)$, then $y \in B_{C_1\delta_y}^{(2)}(y)$ which implies that $y \in \bigcup_{z \in  B_{\epsilon}^{(1)}(x)}B_{C_1\delta_z}^{(2)}(z)$. Moreover, if $y \in \bigcup_{z \in  B_{\epsilon}^{(1)}(x)}B_{C_1\delta_z}^{(2)}(z)$, then $y \in B_{C_1\delta_z}^{(2)}(z)$ for some $z \in B_{\epsilon}^{(1)}(x)$. It follows that
    \begin{align*}
        y \in B_{C_1\delta_z}^{(2)}(z) &\implies \lnorm{2}{z - y} < C_1 \delta_z \\
        &\implies C_1 \lnorm{1}{z - y} < C_1 \delta_z \\
        &\implies \lnorm{1}{z - y} < \delta_z \\
        &\implies y \in B_{\delta_z}^{(1)}(z) \\
        &\implies y \in B_{\epsilon}^{(1)}(x). 
    \end{align*}
    Therefore, $B_{\epsilon}^{(1)}(x)$ can be written as a union of open sets in $\tau_2$, so $B_{\epsilon}^{(1)}(x)$ is itself open in $\tau_2$. Since it holds for all $\epsilon > 0$ and $x \in X$, then $T_1 \subset \tau_2$. Thus, it follows that $\tau_1 \subset \tau_2$. Proving the reverse inclusion is very similar.
\end{proof}

\section{Banach Spaces}

\begin{definition}[Banach Space]
    A NLS $(X, \norm{\cdot})$ is complete if its induced metric space is complete, i.e., every Cauchy sequence converge. We call such a NLS a Banach Space. When it is clear from the context, we can directly write that $X$ is complete or that $X$ is a Banach Space.
\end{definition}

\begin{theorem}[Completeness Criterion]
    Let $(X, \norm{\cdot})$ be a NLS, then $X$ is complete if and only if every absolutely convergent series converges in $X$.
\end{theorem}

\begin{proof}
    ($\implies$) Suppose $X$ is complete and let $\{x_n\}_n$ be a sequence such that
    $$\sum_{k=1}^{\infty}\norm{x_k} < \infty.$$
    Set $S_n = \sum_{k=1}^{n}x_k \in X$ and notice that for all $\epsilon > 0$, by convergence of the series $\sum_{k=1}^{\infty}\norm{x_k}$, there is a $N \in \N$ such that for all $m > n \geq N$, we have
    $$\abs{\sum_{k=1}^{m}\norm{x_k} - \sum_{k=1}^{n}\norm{x_k}} < \epsilon.$$
    But since the norm is positive and $m > n$, then we can simply rewrite the previous inequality as
    $$\sum_{k=n+1}^{m}\norm{x_k} < \epsilon.$$
    Thus, for all $m > n \geq N$, by the triangle inequality,
    $$\norm{S_m - S_n} = \norm{\sum_{k=1}^{m}x_k - \sum_{k=1}^{n}x_k} \leq \sum_{k=n+1}^{m}\norm{x_k} < \epsilon$$
    Therefore, $\{S_n\}_n$ is a Cauchy sequence so by completeness, there is a $x \in X$ such that $\sum_{k=1}^{\infty}x_k = x \in X$. \\
    $( \ \Longleftarrow \ )$ Suppose that any absolutely convergent series converges in $X$, we want to show that $X$ is complete. To do so, let $\{x_n\}_n$ be a Cauchy sequence in $X$, then we can define recursively a sequence of positive integers $\{n_k\}_k$ as follows: Since $\{x_n\}_n$ is Cauchy, then there is an integer $n_1$ such that $\norm{x_m - x_n} \leq 2^{-1}$ for all $m > n \geq n_1$. Similarly, if $n_k$ is defined, then we can define $n_{k+1}$ as follows: Since $\{x_n\}_n$ is Cauchy, then there is an integer $N$ such that $\norm{x_m - x_n} \leq 2^{-(k+1)}$ for all $m > n \geq N$, define $n_{k+1}$ as the maximum between $N$ and $n_k + 1$. Therefore, the sequence $\{n_k\}_k$ is strictly increasing. \\
    From this sequence, define the sequence $\{y_j\}_j$ by $y_1 = x_{n_1}$ and $y_j = x_{n_j} - x_{n_{j-1}}$ for $j \geq 2$. Then by construction, for all positive integers $k$, we have $\sum_{j=1}^{k}y_j = x_{n_k}$. Moreover,
    \begin{align*}
        \sum_{k=1}^{\infty}\norm{y_k} &= \norm{y_1} + \sum_{k=2}^{\infty}\norm{x_{n_k} - x_{n_{k-1}}} \\
        &= \norm{y_1} + \sum_{k=1}^{\infty}\norm{x_{n_{k+1}} - x_{n_k}} \\
        &\leq \norm{y_1} + \sum_{k=1}^{\infty}2^{-k} \\
        &= \norm{y_1} + 1\\
        &< \infty.
    \end{align*}
    So by our assumption, the series $\sum y_k$ converges to a $x \in X$. But since $\sum_{j=1}^{k}y_j = x_{n_k}$ for all $k$, then the sequence $\{x_{n_k}\}_k$ must converge to $x$ as well. Thus a subsequence of $\{x_n\}_n$ converges in $X$ so $\{x_n\}_n$ must converge as well since it is Cauchy. Therefore, $X$ is complete.
\end{proof}

\begin{theorem}[$L^1$ is Banach]
    Let $(X, \M, \mu)$ be a measure space, then $L^1(\mu)$ is Banach.
\end{theorem}

\begin{proof}
    Let's apply the Completeness Criterion stated and proved right above. Suppose $\{f_n\}_n$ is a sequence of functions such that 
    $$\sum_{n=1}^{\infty}\lnorm{1}{f_n} < \infty,$$
    then by the Monotone Convergence Theorem, 
    $$\int_X \sum_{n=1}^{\infty}|f_n|d \mu = \sum_{n=1}^{\infty}\lnorm{1}{f_n} < \infty$$
    so $\sum_{n=1}^{\infty}|f_n|$ is finite a.e. Hence, the function $f : x \mapsto \sum_{n=1}^{\infty}|f_n(x)|$ exists on $X$ (if the series diverges for a $x$, let $f(x)$ be zero). Moreover, by the Monotone Convergence Theorem, $f \in L^1(\mu)$. To show that $f_n \to f$ in $L_1(\mu)$, simply notice that
    \begin{align*}
        \lim_{n \rightarrow \infty}\lnorm{1}{f - f_n} &= \lim_{n \rightarrow \infty}\int_X \abs{\sum_{j=1}^{\infty}|f_j| - \sum_{j=1}^{n}|f_j|} d\mu \\
        &= \lim_{n \rightarrow \infty}\int_X \sum_{j=n+1}^{\infty}|f_j| d\mu \\
        &= \lim_{n \rightarrow \infty} \sum_{j=n+1}^{\infty} \int_X |f_j| d\mu \\
        &= \lim_{n \rightarrow \infty} \sum_{j=n+1}^{\infty} \lnorm{1}{f_j}\\
        &= 0.
    \end{align*}
    Therefore, all absolutely convergent series converge in $X$ so by the Completeness Criterion, $L^1(\mu)$ is Banach.
\end{proof}

\section{Bounded Operators}

\begin{definition}[Boundedness]
    Let $T$ be a linear operator between the NLS $(X, \lnorm{X}{\cdot})$ and $(Y, \lnorm{Y}{\cdot})$. If there is a constant $C > 0$ such that
    $$\lnorm{Y}{Tx} \leq C \lnorm{X}{x}$$
    for all $x \in X$, then we say that $T$ is bounded. The set of all bounded operators from $X$ to $Y$ is denoted by $\L(X,Y)$.
\end{definition}

\begin{theorem}[Equivalence between Continuity and Boundedness]
    Let $T$ be a linear operator between the NLS $(X, \lnorm{X}{\cdot})$ and $(Y, \lnorm{Y}{\cdot})$, then $T$ is continuous if and only if $T$ is bounded.
\end{theorem}

\begin{proof}
    $( \implies )$ Suppose that $T$ is continuous, then for all $y \in X$ and $\epsilon > 0$, there is a $\delta > 0$ such that $\lnorm{X}{Tx - Ty} \leq \epsilon$ for all $x \in X$ satisfying $\lnorm{X}{x - y} \leq \delta$. In particular, for $y = 0$ and $\epsilon = 1$, we get that there exists a $\delta > 0$ such that
    $$\lnorm{X}{x} \leq \delta \implies \lnorm{Y}{Tx} \leq 1$$
    for all $x \in X$. Define $C = 1/\delta$ and notice that for all $x \in X$, we have
    $$\lnorm{X}{\delta\frac{x}{\lnorm{X}{x}}} \leq \delta,$$
    Thus, we obtain:
    \begin{align*}
        \lnorm{Y}{Tx} &= \lnorm{Y}{\frac{\lnorm{X}{x}}{\delta}T\left(\delta\frac{x}{\lnorm{X}{x}}\right)} \\
        &= C\lnorm{X}{x} \lnorm{Y}{T\left(\delta\frac{x}{\lnorm{X}{x}}\right)} \\
        &\leq C \lnorm{X}{x}.
    \end{align*}
    Therefore, $T$ is bounded. \\
    $( \ \Longleftarrow \ )$ Suppose that $T$ is bounded, then there exists a constant $C > 0$ such that
    $$\lnorm{Y}{Tx} \leq C \lnorm{X}{x}$$
    for all $x \in X$. Take an arbitrary $x \in X$, $\epsilon > 0$ and $y \in X$ such that $\lnorm{X}{x - y} \leq \epsilon/C$, then
    $$\lnorm{Y}{Tx - Ty} = \lnorm{Y}{T(x - y)} \leq C\lnorm{X}{x - y} \leq \epsilon.$$
    Therefore, $T$ is continuous.
\end{proof}

\begin{proposition}[$\L$ is a vector space]
    Let $X$ and $Y$ be NLS, then $\L(X,Y)$ is a vector space.
\end{proposition}

\begin{proof}
    Since $\L(X,Y)$ is a subset of the vector space $Y^X$, it suffices to show that $\L(X,Y)$ is a subspace. First, it is non-empty since the constant zero function is a bounded linear operator. Moreover, given $T_1, T_2 \in \L(X,Y)$, there exist constants $C_1, C_2 > 0$ such that
    $$\norm{T_1x} \leq C_1 \norm{x} \quad \text{ and } \quad \norm{T_2x} \leq C_2 \norm{x}$$
    for all $x \in X$. Thus, for all $x \in X$, we have
    $$\norm{(T_1 + T_2)x} \leq \norm{T_1x} + \norm{T_2x} \leq (C_1 + C_2)\norm{x}.$$
    Hence, $T_1 + T_2$ is bounded so $\L(X,Y)$ is closed under addition. Finally, for all $\lambda \in \F$ and $T \in \L(X,Y)$, there exists a constant $C > 0$ such that
    $$\norm{Tx} \leq C \norm{x}$$
    for all $x \in X$. If $\lambda = 0$, then $\lambda T = 0 \in \L(X,Y)$, and if $\lambda \neq 0$, then for all $x \in X$, 
    $$\norm{\lambda Tx} \leq |\lambda| C \norm{x}.$$  
    Hence, $\lambda T $ is bounded so $\lambda T \in \L(X,Y)$. Therefore, $\L(X,Y)$ is a vector space since it is a subspace.  
\end{proof}

\begin{definition}[Operator Norm]
    Given two NLS $X$ and $Y$, we can define the operator norm on $\L(X,Y)$ as follows:
    $$\norm{T} = \sup_{\norm{x} = 1}\norm{Tx} = \sup_{x \neq 0}\frac{\norm{Tx}}{\norm{x}}$$
    for all $T \in \L(X,Y)$.
\end{definition}

\begin{proposition}[$\L$ is a NLS]
    Let $X$ and $Y$ be NLS, then $\L(X,Y)$ is a NLS with the operator norm.
\end{proposition}

\begin{proof}
    We already proved that $\L(X,Y)$ is a vector space so it suffices to prove that the operator norm is indeed a norm on $\L(X,Y)$. To do so, let's first show that $\norm{T} = 0$ if and only if $T = 0$. If $T = 0$, then obviously:
    $$\norm{T} = \sup_{\norm{x} = 1}\norm{0 \cdot x} = 0.$$
    Suppose now that $\norm{T} = 0$ and let $\{b_i\}_{i \in I}$ be a basis for $X$ (which exists by the axiom of choice), then we can assume that all $b_i$'s have norm 1 (otherwise, replace $b_i$ by $b_i / \norm{b_i}$). Hence, for all $i \in I$, we have
    $$\sup_{\norm{x} = 1}\norm{Tx} = 0 \implies \norm{T b_i} = 0 \implies Tb_i = 0.$$
    Since $T$ is identically zero on a basis of $X$, then $T = 0$. This proves the first norm axiom for the operator norm. \\
    Next, take $\lambda \in \F$ and $T \in \L(X,Y)$, then by properties of the supremum, we have
    \begin{align*}
        \norm{\lambda T} &= \sup_{\norm{x} = 1}\norm{\lambda T x} \\
        &= \sup_{\norm{x} = 1}|\lambda|\norm{T x} \\
        &= \lambda \sup_{\norm{x} = 1}\norm{T x} \\
        &= \lambda \norm{T}.
    \end{align*}
    Finally, for all $T_1, T_2 \in \L(X,Y)$ and $x \in X$ with $\norm{x} = 1$, we know that
    $$T_1 x \leq \sup_{\norm{x} = 1}\norm{T_1 x} = \norm{T_1}$$
    and 
    $$T_2 x \leq \sup_{\norm{x} = 1}\norm{T_2 x} = \norm{T_2}.$$
    Thus,
    $$(T_1 + T_2)x = T_1x + T_2x \leq \norm{T_1} + \norm{T_2}.$$
    Since it holds for all $x \in X$ with $\norm{x} = 1$, then
    $$\norm{T_1 + T_2} \leq \norm{T_1} + \norm{T_2}.$$
    Therefore, $\L(X,Y)$ is a NLS. 
\end{proof}

\begin{proposition}
    Let $X$ and $Y$ be NLS, $T \in \L(X,Y)$ and $x \in X$, then
    $$\norm{Tx} \leq \norm{T}\norm{x}$$
\end{proposition}

\begin{proof}
    If $x = 0$, it is trivial. Hence, assume that $x \neq 0$, then
    $$\frac{\norm{Tx}}{\norm{x}} \leq \sup_{y \neq 0}\frac{\norm{Ty}}{\norm{y}} = \norm{T}.$$
    Thus, multiplying on both sides by $\norm{x}$ gives us
    $$\norm{Tx} \leq \norm{T}\norm{x}.$$
\end{proof}

\begin{theorem}[$\L$ is Banach]
    Assume $X$ is a NLS and $Y$ is Banach, then $\L(X,Y)$ is Banach. 
\end{theorem}

\begin{proof}
    Let $\{T_n\}_n$ be a Cauchy sequence in $\L(X,Y)$, then for all $\epsilon > 0$, there is a natural number $N$ such that $\norm{T_m - T_n} < \epsilon$ for all $m > n \geq N$. Thus, for all $x \in X$ and $\epsilon > 0$, there is a natural number $N$ such that $\norm{T_m - T_n} < \epsilon/\norm{x}$ for all $m > n \geq N$. Hence, for all $m > n \geq N$, we obtain
    $$\norm{T_m x - T_n x} \leq \norm{T_m - T_n} \norm{x} < \epsilon.$$
    Therefore, $\{T_nx\}_n$ is a Cauchy sequence in $Y$ for all $x \in X$. Thus, by completeness of $Y$, define the function $T : X \to Y$ such that $Tx = \lim_n T_n x$ for all $x \in X$. Using properties of limits and the linearity of the $T_n$'s, we easily obtain that $T$ is linear. \\
    Let's show that $T$ is bounded. To do so, we will prove that $\lim_n \|T_n - T\| = 0$. \td 
\end{proof}

\begin{proposition}
    Let $X,Y,Z$ be NLS, $T \in \L(X,Y)$ and $S \in \L(Y,Z)$, then $$\|ST\| \leq \|S\|\|T\|.$$
\end{proposition}

\begin{proof}
    Simply notice that 
    $$\|ST\| = \sup_{\|x\| = 1}\|STx\| \leq \sup_{\|x\| = 1}\|S\|\|Tx\| = \|S\| \|T\|.$$
\end{proof}

\begin{definition}[Invertible]
    Let $T \in \L(X,Y)$, we say that $T$ is invertible if it is a bijection and if $T^{-1} \in \L(Y,X)$.
\end{definition}

\begin{definition}[Isometry]
    Let $T \in \L(X,Y)$, we say that $T$ is an isometry if it is invertible and $\|Tx\| = \|x\|$ for all $x \in X$.
\end{definition}

\section{Linear Functionals}
\begin{definition}[Linear Functionals]
    Let $X$ be a vector space, then we define $X^*_{\F}$ as the set of all linear transformations from $X$ to $\F$ and call it the space of $\F$-linear functionals.
\end{definition}

\begin{proposition}
    Let $X$ be a $\C$-vector space and let $f : X \to \C$ be a $\C$-linear functional, then $u = \Re f$ is a $\R$-linear functional and $f(x) = u(x) - iu(ix)$ for all $x \in X$. Conversely, if $u$ is $\R$-linear functional, then we can construct a $\C$-linear functional $f : X \to \C$ as follows: $f(x) = u(x) - iu(ix)$. Finally, if $X$ is a NLS, $\norm{u} = \norm{f}$.
\end{proposition}

\begin{proof}
    $(\implies)$ Let $f$ be a $\C$-linear functional, then $f$ is $\R$-linear. It follows that $u = \Re f$ is $\R$-linear as well so $u$ is a $\R$-linear functional. Moreover, using the identity $\Im z = - \Re(iz)$, we get that
    $$f(x) = \Re (f(x)) + i \Im (f(x)) = u(x) - i u(ix).$$
    $( \ \Longleftarrow \ )$ Suppose now that $u$ is a $\R$-linear functional, set $f(x) = u(x) - iu(ix)$ for all $x \in X$, then by $\R$-linearity of $u$, we have that $f$ is $\R$-linear as well. To prove that it is $\C$-linear, it suffices to prove that $f(ix) = if(x)$:
    \begin{align*}
        f(ix) &= u(ix) - iu(i(ix)) \\
        &= u(ix) + iu(x) \\
        &= i(u(x) - iu(ix)) \\
        &= if(x).
    \end{align*}
    Therefore, $f$ is a $\C$-linear functional.

    Finally, suppose that $X$ is a NLS, then we easily get that
    $$\|u\| = \sup_{\|x\| = 1}|u(x)| \leq \sup_{\|x\| = 1}|f(x)| \leq \|f\|.$$
    Let's prove the reverse inequality. If $f(x) \neq 0$, define $\alpha = \frac{\overline{f(x)}}{|f(x)|}$ and notice that,
    $$|\alpha| = \abs{\frac{\overline{f(x)}}{|f(x)|}} = \frac{|f(x)|}{|f(x)|} = 1.$$
    Moreover, by $\C$-linearity, we have
    $$f(\alpha x) = \alpha f(x) = \frac{\overline{f(x)}}{|f(x)|}f(x) = \frac{|f(x)|^2}{|f(x)|} = |f(x)|.$$
    Therefore, since $f(\alpha x) \in \R$, then 
    $$u(\alpha x) = f(\alpha x) = |f(x)|.$$
    It follows that
    $$\norm{f} = \sup_{\|x\| = 1}|f(x)| = \sup_{\|x\| = 1} u(\alpha x) \leq \sup_{\|x\| = 1} |u(\alpha x)| = \|u\|.$$
    The last inequality follows from the fact that $\|\alpha x\| = |\alpha| \norm{x} = \norm{x}$ so 
    $$\{|u(\alpha x)| : \norm{x} = 1\} = \{|u(x)| : \norm{x} = 1\}.$$
    Therefore, $\norm{u} = \norm{f}$.
\end{proof}

\begin{definition}[Sublinear Functional]
    Let $X$ be a $\R$-vector space. A sublinear functional is a map $p : X \to \R$ satisfying
    \begin{enumerate}
        \item $p(x+y) \leq p(x) + p(y)$
        \item $p(\lambda x) = \lambda p(x)$ where $\lambda > 0$.
    \end{enumerate}  
\end{definition}

\begin{theorem}[Hahn-Banach Real Version]
    Let $X$ be a $\R$-vector space, $p$ be a sublinear functional, $M$ be a subspace of $X$ and $f : M \to \R$ be a $\R$-linear functional satisfying $f \leq p$, then there exists a $\R$-linear extension $F : X \to \R$ with $F \leq p$ and $F|_M = f$.
\end{theorem}

\begin{proof}
    First, let $x \in X \setminus M$ and notice that for all $y_1, y_2 \in M$, we have
    $$f(y_1) + f(y_2) = f(y_1 + y_2) \leq p(y_1 + y_2) \leq p(y_1 - x) + p(x + y_2)$$
    which implies that
    $$f(y_1) - p(y_1 - x) \leq p(x + y_2) - f(y_2).$$
    Since it holds for all $y_1, y_2 \in M$, we get that
    $$\sup_{y \in M}\{f(y) - p(y - x)\} \leq \inf_{y \in M}\{p(x + y) - f(y)\}.$$
    From this, define $\alpha$ to be a real number satisfying
    $$\sup_{y \in M}\{f(y) - p(y - x)\} \leq \alpha \leq \inf_{y \in M}\{p(x + y) - f(y)\},$$
    and consider the function $g : M + \R x \to \R$ defined by $g(y + \lambda x) = f(y) + \lambda \alpha$. Since $x \notin M$, then $g$ is well-defined. Moreover, it is clear that $g$ is linear and that it extends $f$ (by letting $\lambda = 0$).

    Let's prove that $g \leq p$ by cases on the sign of $\lambda$. When $\lambda = 0$, it follows from the fact that $f \leq p$. When $\lambda > 0$, we can use the fact that
    $$\alpha \leq \inf_{y \in M}\{p(x + y) - f(y)\} \leq p\left(x + \frac{y}{\lambda}\right) - f\left(\frac{y}{\lambda}\right)$$
    to get that
    \begin{align*}
        g(y + \lambda x) &= \lambda \left[f\left(\frac{y}{\lambda}\right) + \alpha\right] \\
        &\leq \lambda \left[f\left(\frac{y}{\lambda}\right) + p\left(x + \frac{y}{\lambda}\right) - f\left(\frac{y}{\lambda}\right)\right] \\
        &= p(y + \lambda x).
    \end{align*}
    When $\lambda < 0$, we can use the fact that
    $$f\left(\frac{y}{-\lambda}\right) - p\left(\frac{y}{-\lambda} - x\right) \leq \sup_{y \in M}\{f(y) - p(y - x)\} \leq \alpha$$
    to get that
    \begin{align*}
        g(y + \lambda x) &= -\lambda \left[f\left(\frac{y}{-\lambda}\right) - \alpha\right] \\
        &\leq -\lambda \left[f\left(\frac{y}{-\lambda}\right) - f\left(\frac{y}{-\lambda}\right) + p\left(\frac{y}{-\lambda} - x\right) \right] \\
        &= p(y + \lambda x).
    \end{align*}
    Therefore, for all $y + \lambda x \in M + \R x$, we have that $g \leq p$. \\
    The goal now is to repeat this process infinitely many times. To do this rigorously, we will use Zorn's Lemma. Let $\mathcal{F}$ be the set of all linear extensions $F$ of $f$ such that $F \leq p$ and $F$ is defined on a subspace of $X$. View each elements of $\mathcal{F}$ as subsets of $X \times \R$ instead of linear functionals, then it follows that $\mathcal{F}$ is partially ordered by inclusion. Moreover, any chain has a maximal elements (which can be obtained by taking the union of the chain). It follows by Zorn's Lemma that $\mathcal{F}$ has a maximal element $F$. Hence, there exists a linear functional $F$ on a subspace $U$ of $X$ that extends $f$ and that satisfies $F \leq p$ such that no linear functional with the same properties extends $F$ and is defined on a larger subspace. However, if $U$ is different than $X$, then we can create such an extension of $F$ on $U + \R x_0$ (for some $x_0 \in X\setminus U$) as we did in the first part of this proof. But this would contradict the fact that $F$ is maximal. Therefore, $F$ is defined on $V$.
\end{proof}

\begin{theorem}[Hahn-Banach Complex Version]
    Let $X$ be a $\C$-vector space, $p$ be a semi-norm, $M$ be a subspace of $X$ and $f : M \to \C$ be a $\C$-linear functional satisfying $|f| \leq p$, then there exists a $\C$-linear extension $F : X \to \C$ with $|F| \leq p$ and $F|_M = f$.
\end{theorem}

\begin{proof}
    Let $u$ be the real part of $f$, then by the Real Version of Hahn-Banach, there exists a $\R$-linear functional $U$ that extends $u$ such that $U \leq p$. But notice that for all $x \in X$, since $U \leq p$ holds on $X$, then in particular, $U(x) \leq p(x)$ and $U(-x) \leq p(-x)$. Moreover, since $p$ is a semi-norm and $U$ is linear, then $U(-x) \leq p(-x)$ is equivalent to $-p(x) \leq U(x)$. Combining these inequalities, we get that $|U| \leq p$ holds on $X$. Thus, by a Proposition we proved earlier, we can construct from $U$ a $\C$-linear functional $F$ by
    $$F(x) = U(x) - iU(ix).$$
    when $x \in M$, we get that
    $$F(x) = U(x) - iU(ix) = u(x) - iu(ix) = f(x)$$
    so $F$ extends $f$. Moreover, for all $x \in X$, if $|F(x)| = 0$, then $|F(x)| \leq p(x)$, and if $|F(x)| \neq 0$, we can define $\alpha = \frac{\overline{F(x)}}{|F(x)|}$ to get that 
    $$|F(x)| = \alpha F(x) = F(\alpha x) \in \R$$
    which implies that
    $$|F(x)| = F(\alpha x) = U(\alpha x) \leq p(\alpha x) = |\alpha| p(x) = p(x).$$
    Therefore, $F$ is a $\C$-linear functional that extends $f$ and that satisfies $|F| \leq p$.
\end{proof}

\begin{theorem}
    Let $X$ be a NLS.
    \begin{enumerate}[label=(\alph*)]
        \item Suppose $M$ is closed subspace of $X$ and let $x \in X \setminus M$, then there exists a $f \in X^*$ such that $f(x) = \inf_{y\in M}\norm{x - y} > 0$, $f|_M = 0$ and $\norm{f} = 1$.
        \item If $x \neq 0$, there exists a $f \in X^*$ such that $\norm{f} = 1$ and $f(x) = \norm{x}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Define $\delta = \inf_{y\in M}\norm{x - y}$, if $\delta = 0$, then $x$ is a limit-point of $M$ which implies that $x \in M$ (using the fact that $M$ is closed). But this is a contradiction with the fact that $x \in X\setminus M$. Thus, $\delta > 0$. Define $g : M + \C x \to \C$ by $g(y+\lambda x) = \lambda \delta$. Clearly, $g$ is a $\C$-linear functional on $M + \lambda x$. Moreover, notice that for all $y + \lambda x \in M + \C x$, we have
        $$|g(y + \lambda x)| = |\lambda|\inf_{y\in M}\norm{x - y} \leq |\lambda| \norm{x - \left(-\frac{y}{\lambda}\right)} = \norm{y + \lambda x}.$$
        Thus, we can apply the Complex Version of the Hahn-Banach Theorem with $p = \norm{\cdot}$ to get that there exists a linear function $f : X \to \C$ such that $f$ extends $g$ and $|f(z)| \leq \norm{z}$ for all $z \in X$. It follows that for all $y \in M$, $f(y) = g(y) = 0$ so $f|_M = 0$. Moreover, $f(x) = g(x) = \delta$. Finally, let's prove that $\norm{f} = 1$. First,
        $$\norm{f} = \sup_{z \neq 0}\frac{\abs{fz}}{\norm{z}} \leq \sup_{z \neq 0}\frac{\norm{z}}{\norm{z}} = 1.$$
        To prove the reverse inequality, take $z = x/\delta$ and notice that $|fz|/\norm{z} = 1$ \td : why ?? Therefore, $\norm{f} = 1$.
        \item It follows from part (a) by taking $M = \{0\}$.
    \end{enumerate}
\end{proof}

\subsection{Baire Category Theorem}

\begin{definition}[Meager Sets]
    A topological space is said to be meager if it is a countable union of nowhere-dense sets. We can also say that it is of first category.
\end{definition}

\begin{theorem}[Baire Category Theorem]
    Let $X$ be a complete metric space.
    \begin{enumerate}[label=(\alph*)]
        \item If $\{U_n\}$ is a countable collection of open dense sets in $X$, then $\overline{\cap_{n=1}^{\infty}U_n} = X$.
        \item $X$ is not meager.
    \end{enumerate}
\end{theorem}

\end{document}